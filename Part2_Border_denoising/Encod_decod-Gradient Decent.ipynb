{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intraprediction Project\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENCODER DECODER\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "from optparse import OptionParser\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "from PIL import Image\n",
    "from torch.autograd import Function, Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "import glob\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ TODO 1 ] First define following layers to be used later\n",
    "- **Conv2d + BatchNorm2d + ReLu ** as **single conv2d layer** ,\n",
    "- **Maxpool2d + single conv2d layer ** as **down layer**,\n",
    "- **Upsample + single conv2d layer ** as **up layer** ,\n",
    "-  **Conv2d ** as **outconv layer** \n",
    "\n",
    "You can check out the documentation in this link to understand how to use the methods called in the provided template:\n",
    "\n",
    " https://pytorch.org/docs/stable/nn.html\n",
    " \n",
    "  ![single_conv](single_conv_layer.png)\n",
    "  ![down_layer](down_layer.png)\n",
    "  ![up_layer](Up_layer.png)\n",
    "  ![outconv_layer](outconv_layer.png)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################ [TODO] ###################################################\n",
    "# DEFINE SINGLE_CONV CLASS\n",
    "class single_conv(nn.Module):\n",
    "    '''(conv => BN => ReLU) '''\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(single_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_ch), # Channels in\n",
    "            nn.Conv2d(in_ch,out_ch,3,padding = (1,1)), #(channel in, channel out, filter)\n",
    "            nn.Tanh(), #Channel in\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #paddings = tf.constant([[0, 0,],[0, 0,],[1, 1,], [1, 1]])\n",
    "        #x = tf.pad(x, paddings, \"SYMMETRIC\")\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "################################################ [TODO] ###################################################\n",
    "# DEFINE DOWN CLASS\n",
    "class down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(down, self).__init__()\n",
    "        self.down = nn.Conv2d(in_ch,in_ch,kernel_size=3,stride =2) #(channel in, channel out, filter)\n",
    "        self.conv = single_conv(in_ch, out_ch) # use previously defined single_cov\n",
    "    def forward(self, x):\n",
    "        x = self.down(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "################################################ [TODO] ###################################################\n",
    "# DEFINE UP CLASS\n",
    "# Note that this class will not only upsample x1, but also concatenate up-sampled x1 with x2 to generate the final output\n",
    "\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(up, self).__init__()       \n",
    "        self.up = nn.Upsample(2,'bilinear') # use nn.Upsample( )\n",
    "        self.conv = single_conv(in_ch,out_ch) # use previously defined single_cov\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This part is tricky, so we provide it for you\n",
    "        # First we upsample x1\n",
    "        x = nn.functional.interpolate(x, size=None, scale_factor=2, mode='bilinear')\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "class resize(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(resize, self).__init__()       \n",
    "        self.up = nn.Upsample(2,'bilinear') # use nn.Upsample( )\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_ch), # Channels in\n",
    "            nn.Conv2d(in_ch,out_ch,3), #(channel in, channel out, filter)\n",
    "            nn.Tanh(), #Channel in\n",
    "        )\n",
    "    def forward(self, x, siz):\n",
    "        x = nn.functional.interpolate(x, size=(siz+2,siz+2),  mode='bilinear')\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "################################################ [TODO] ###################################################\n",
    "# DEFINE OUTCONV CLASS\n",
    "class quat(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(quat, self).__init__()\n",
    "        self.down = down(in_ch,out_ch)\n",
    "    def forward(self, x):\n",
    "        x = self.down(x)\n",
    "        #Quatization\n",
    "        T = 1\n",
    "        x = (x > T).float() * 1\n",
    "        return x\n",
    "\n",
    "class upT(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(upT, self).__init__()\n",
    "        self.up =  nn.Sequential(\n",
    "            nn.BatchNorm2d(in_ch), # Channels in\n",
    "            nn.ConvTranspose2d(in_ch,out_ch,kernel_size=4, stride=2), #(channel in, channel out, filter)\n",
    "            nn.Tanh(), #Channel in\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################ [TODO] ###################################################\n",
    "# Build your network with predefined classes: single_conv, up, down, outconv\n",
    "# The number of input and output channels should follow the U-Net Structure shown above.\n",
    "import torch.nn.functional as F\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels):\n",
    "        super(UNet, self).__init__()\n",
    "        self.inc = single_conv(n_channels, 6) # conv2d +  batchnorm + relu \n",
    "        self.down1 = resize(6, 8)              # maxpool2d + conv2d + batchnorm + relu\n",
    "        self.down2 = resize(8, 8)              # maxpool2d + conv2d + batchnorm + relu\n",
    "        self.down3 = resize(8,8)                # maxpool2d + conv2d + batchnorm + relu +Quatization\n",
    "        self.up1 = upT(8,6)                   # upsample + pad + conv2d + batchnorm + relu\n",
    "        self.up2 = upT(6,3)             # conv2d\n",
    "        self.upF =  nn.Sequential(nn.Conv2d(3,3,3), #(channel in, channel out, filter)\n",
    "            nn.Tanh(), #Channel in\n",
    "        )\n",
    "        self.upF2 =  nn.Sequential(nn.Conv2d(8,8,2), #(channel in, channel out, filter)\n",
    "            nn.Tanh(), #Channel in\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        #ENCODE\n",
    "        x = self.inc(x) #16x16\n",
    "        x = self.down1(x,12) #12x12\n",
    "        x = self.down2(x,8) #8x8\n",
    "        x = self.down3(x,4) #4x4\n",
    "        x = self.upF2(x) #3x3\n",
    "        #x = self.quat(x) \n",
    "        #DECODE\n",
    "        x = self.up1(x) #8x8\n",
    "        x = self.up2(x) #18x18\n",
    "        #x = self.up3(x)\n",
    "        x = self.upF(x) #16x16\n",
    "        #x = x[:,:,8:24,8:24];\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ TODO 2 ] Define evaulation function:\n",
    "Based on what we have learnt in class, Dice coefficient is defined as \n",
    "![dice.png](dice.png)\n",
    "For the case of evaluating a Dice coefficient on predicted segmentation masks, we can approximate intersection of A and B as the element-wise multiplication between the prediction and target mask, and then sum the resulting matrix.\n",
    "\n",
    "In order to quantify the area of A union B, some researchers use the simple sum whereas other researchers prefer to use the squared sum for this calculation. You can use either way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################ [TODO] ###################################################\n",
    "# define dice coefficient \n",
    "class MseCoeff(Function):\n",
    "    \"\"\"Dice coeff for one pair of input image and target image\"\"\"\n",
    "    def forward(self, input, target):\n",
    "        self.save_for_backward(input, target)\n",
    "        ################################################ [TODO] ###################################################\n",
    "        # Calculate intersection and union. \n",
    "        # You can convert the input image into a vector with input.contiguous().view(-1)\n",
    "        # Then use torch.dot(A, B) to calculate the intersection.\n",
    "        # Use torch.sum(A) to get the sum.\n",
    "        input = input.contiguous().view(-1)\n",
    "        target = target.contiguous().view(-1)\n",
    "        # Calculate Mean Squared Error\n",
    "        diff = input - target;\n",
    "        dot =  torch.dot(diff,diff)\n",
    "        dot = dot/input.numel()\n",
    "        return dot\n",
    "\n",
    "\n",
    "################################################ [TODO] ###################################################\n",
    "# Calculate dice coefficients for batches\n",
    "def mse_coeff(input, target):\n",
    "    \"\"\"Dice coeff for batches\"\"\"\n",
    "    s = torch.FloatTensor(1).zero_()    \n",
    "    # For each pair of input and target, call DiceCoeff().forward(input, target) to calculate dice coefficient\n",
    "    # Then average\n",
    "    MSE = MseCoeff()\n",
    "    for i, (c, b) in enumerate(zip(input, target)):\n",
    "        s += MSE.forward(c,b);\n",
    "    s = s / (i + 1)\n",
    "    return s\n",
    "\n",
    "def grad(input):\n",
    "    x_right = F.pad(input, (0, 1, 0, 0))\n",
    "    x_left = F.pad(input, (1, 0, 0, 0 ))\n",
    "    y_up =   F.pad(input, (0, 0, 1, 0 )) \n",
    "    y_down =   F.pad(input, (0, 0, 0,1))\n",
    "    dx = x_right - x_left;\n",
    "    dy = y_up - y_down;\n",
    "    return dx[:,1:16, 1:16], dy[:,1:16, 1:16]\n",
    "\n",
    "def grad_loss(input, target, epoch):\n",
    "    s = torch.FloatTensor(1).zero_()    \n",
    "    # For each pair of input and target, call DiceCoeff().forward(input, target) to calculate dice coefficient\n",
    "    # Then average\n",
    "   # print(input.shape)\n",
    "   # print(target.shape)\n",
    "    MSE = MseCoeff()\n",
    "    s = 0;\n",
    "    if epoch == 0:\n",
    "        p,q,r = 0.8, 0.2, 0.2\n",
    "    elif epoch == 1: \n",
    "        p,q,r  = 0.6, 0.4, 0.4\n",
    "    elif epoch ==2:\n",
    "        p,q,r  = 0.4,0.6, 0.6\n",
    "    else:\n",
    "        p,q,r = 0.2,0.8,0.8\n",
    "    for i, (c, b) in enumerate(zip(input, target)): \n",
    "        Gx_in , Gy_in = grad(c)\n",
    "        Gx_target, Gy_target = grad(b)\n",
    "        s += p*MSE.forward(c,b);\n",
    "        s += q*MSE.forward(Gx_in,Gx_target);\n",
    "        s += r*MSE.forward(Gy_in, Gy_target);\n",
    "    s = s / (i+1)\n",
    "    return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = glob.glob('./Data/Train/train_16_2')\n",
    "# for i in range(len(train)):\n",
    "#     f=open(train[i],'rb')\n",
    "#     b=pickle.load(f)\n",
    "#     b = np.transpose(b, axes=[3, 2, 0, 1]) /255.\n",
    "#     f.close()\n",
    "#     trainDataset.append(b)\n",
    "# k=trainDataset[0]\n",
    "# print(k.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load images and masks\n",
    "\n",
    "Load training imagse, normalize and resize them into smaller size so that you can perform training using a CPU. Split them into training and validation. Validation percent of 0.05 means 5% of training dataset is used as validation. In order to avoid repeated data preprocessing, we use the pickle tool to save the processed data.\n",
    "\n",
    "This part has been done for you. But please read through so that you learn the general processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = glob.glob('./Data/Validation/validation_16_2')\n",
    "train = glob.glob('./Data/Train/train_16_2')\n",
    "test = glob.glob('./Data/Test/*')\n",
    "valiDataset = []\n",
    "trainDataset = []\n",
    "testDataset = []\n",
    "#Loads the following in a list of (3, 16,16) and (3, 20,20)\n",
    "for i in range(len(validation)):\n",
    "    f=open(validation[i],'rb')\n",
    "    b=pickle.load(f)\n",
    "    #flip the data dimension\n",
    "    b = np.transpose(b, axes=[3, 2, 0, 1])\n",
    "    f.close()\n",
    "    valiDataset.append(b)\n",
    "    \n",
    "for i in range(len(test)):\n",
    "    f=open(test[i],'rb')\n",
    "    b=pickle.load(f)\n",
    "    b = np.transpose(b, axes=[3, 2, 0, 1])\n",
    "    f.close()\n",
    "    testDataset.append(b)\n",
    "    \n",
    "for i in range(len(train)):\n",
    "    f=open(train[i],'rb')\n",
    "    b=pickle.load(f)\n",
    "    b = (np.transpose(b, axes=[3, 2, 0, 1])-128)/255.\n",
    "    f.close()\n",
    "    trainDataset.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Let us display some of the images to make sure the data loading and processing is correct.\n",
    "### the original size of the image is: 1280*1918, but we resize the image to 80*100 for \n",
    "### training the segmentation network\n",
    "img_num =56\n",
    "a = valiDataset[0][img_num]\n",
    "a = np.transpose(a, axes = [1,2,0])+128/255.\n",
    "plt.imshow(a)\n",
    "plt.title(\"sample image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datasets in the format that you can later use Torch  \"DataLoader\" during training and define  data augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### define transform classes for  data augmentation\n",
    "class Flip(object):\n",
    "    \"\"\"\n",
    "    Flip the image left or right for data augmentation, but prefer original image.\n",
    "    \"\"\"\n",
    "    def __init__(self,ori_probability=0.60):\n",
    "        self.ori_probability = ori_probability\n",
    " \n",
    "    def __call__(self, sample):\n",
    "        if random.uniform(0,1) < self.ori_probability:\n",
    "            return sample\n",
    "        else:\n",
    "            img, label = sample['img'], sample['label']\n",
    "            img_flip = img[:,:,::-1]\n",
    "            label_flip = label[:,::-1]\n",
    "            \n",
    "            return {'img': img_flip, 'label': label_flip}\n",
    "        \n",
    "class ToTensor(object):\n",
    "    \"\"\"\n",
    "    Convert ndarrays in sample to Tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['img'], sample['label']\n",
    "\n",
    "        return {'img': torch.from_numpy(image.copy()).type(torch.FloatTensor),\n",
    "                'label': torch.from_numpy(label.copy()).type(torch.FloatTensor)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, image_masks,  transforms=None):   # initial logic happens like transform\n",
    "        self.image = images\n",
    "        self.image_masks = image_masks\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __len__(self):  # return count of sample we have\n",
    "\n",
    "        return len(self.image_masks)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        orig = self.image[index];\n",
    "        image = self.image_masks[index]\n",
    "        sample = {'img': orig, 'label': image}\n",
    "        if transforms:\n",
    "            sample = self.transforms(sample)            \n",
    "        return sample\n",
    "\n",
    "#train_dataset = CustomDataset(((trainDataset[1]-128)/255), trainDataset[0], transforms=transforms.Compose([Flip(),ToTensor()]))\n",
    "train_dataset = CustomDataset(trainDataset[0], trainDataset[0], transforms=transforms.Compose([Flip(),ToTensor()]))\n",
    "val_dataset = CustomDataset(valiDataset[0], valiDataset[0], transforms=transforms.Compose([Flip(),ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ TODO 3 ] Start training your network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################ [TODO] ###################################################\n",
    "# Create a UNET object. Input channels = 3, output channels = 1\n",
    "net = UNet(3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################ [TODO] ###################################################\n",
    "# This function is used to evaluate the network after each epoch of training\n",
    "# Input: network and validation dataset\n",
    "# Output: average dice_coeff\n",
    "def eval_net(net, dataset):\n",
    "    # set net mode to evaluation\n",
    "    net.eval()\n",
    "    tot = 0\n",
    "    for i, b in enumerate(dataset):\n",
    "        img = b['img']\n",
    "        label = b['label']\n",
    "        img = img.type(torch.FloatTensor);\n",
    "        ################################################ [TODO] ################################################### \n",
    "        # Feed in the image to get predicted mask\n",
    "        pred_img = net.forward(img)\n",
    "        tot += mse_coeff(pred_img,label);\n",
    "    return tot / (i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/2.\n",
      "0.0000 --- loss: 0.101588\n",
      "0.0007 --- loss: 0.100166\n",
      "0.0014 --- loss: 0.098531\n",
      "0.0021 --- loss: 0.091888\n",
      "0.0029 --- loss: 0.071475\n",
      "0.0036 --- loss: 0.092463\n",
      "0.0043 --- loss: 0.079295\n",
      "0.0050 --- loss: 0.079214\n",
      "0.0057 --- loss: 0.097224\n",
      "0.0064 --- loss: 0.085393\n",
      "0.0071 --- loss: 0.082472\n",
      "0.0079 --- loss: 0.075689\n",
      "0.0086 --- loss: 0.081939\n",
      "0.0093 --- loss: 0.067288\n",
      "0.0100 --- loss: 0.084221\n",
      "0.0107 --- loss: 0.062104\n",
      "0.0114 --- loss: 0.080646\n",
      "0.0121 --- loss: 0.075553\n",
      "0.0129 --- loss: 0.072563\n",
      "0.0136 --- loss: 0.063045\n",
      "0.0143 --- loss: 0.063743\n",
      "0.0150 --- loss: 0.071619\n",
      "0.0157 --- loss: 0.068570\n",
      "0.0164 --- loss: 0.068328\n",
      "0.0171 --- loss: 0.064777\n",
      "0.0179 --- loss: 0.081369\n",
      "0.0186 --- loss: 0.058422\n",
      "0.0193 --- loss: 0.071940\n",
      "0.0200 --- loss: 0.053395\n",
      "0.0207 --- loss: 0.073425\n",
      "0.0214 --- loss: 0.083818\n",
      "0.0221 --- loss: 0.061390\n",
      "0.0229 --- loss: 0.059092\n",
      "0.0236 --- loss: 0.049758\n",
      "0.0243 --- loss: 0.052464\n",
      "0.0250 --- loss: 0.063278\n",
      "0.0257 --- loss: 0.060950\n",
      "0.0264 --- loss: 0.064747\n",
      "0.0271 --- loss: 0.066075\n",
      "0.0279 --- loss: 0.058023\n",
      "0.0286 --- loss: 0.056524\n",
      "0.0293 --- loss: 0.058486\n",
      "0.0300 --- loss: 0.049709\n",
      "0.0307 --- loss: 0.058622\n",
      "0.0314 --- loss: 0.069031\n",
      "0.0321 --- loss: 0.044876\n",
      "0.0329 --- loss: 0.054203\n",
      "0.0336 --- loss: 0.049448\n",
      "0.0343 --- loss: 0.048997\n",
      "0.0350 --- loss: 0.040062\n",
      "0.0357 --- loss: 0.049399\n",
      "0.0364 --- loss: 0.042366\n",
      "0.0371 --- loss: 0.049503\n",
      "0.0379 --- loss: 0.051797\n",
      "0.0386 --- loss: 0.043567\n",
      "0.0393 --- loss: 0.043517\n",
      "0.0400 --- loss: 0.048826\n",
      "0.0407 --- loss: 0.046125\n",
      "0.0414 --- loss: 0.036181\n",
      "0.0421 --- loss: 0.038791\n",
      "0.0429 --- loss: 0.034646\n",
      "0.0436 --- loss: 0.035707\n",
      "0.0443 --- loss: 0.036915\n",
      "0.0450 --- loss: 0.029687\n",
      "0.0457 --- loss: 0.031607\n",
      "0.0464 --- loss: 0.035689\n",
      "0.0471 --- loss: 0.037214\n",
      "0.0479 --- loss: 0.041060\n",
      "0.0486 --- loss: 0.037424\n",
      "0.0493 --- loss: 0.029511\n",
      "0.0500 --- loss: 0.038121\n",
      "0.0507 --- loss: 0.024589\n",
      "0.0514 --- loss: 0.027819\n",
      "0.0521 --- loss: 0.028746\n",
      "0.0529 --- loss: 0.024721\n",
      "0.0536 --- loss: 0.032521\n",
      "0.0543 --- loss: 0.037361\n",
      "0.0550 --- loss: 0.024627\n",
      "0.0557 --- loss: 0.024569\n",
      "0.0564 --- loss: 0.025747\n",
      "0.0571 --- loss: 0.027536\n",
      "0.0579 --- loss: 0.027519\n",
      "0.0586 --- loss: 0.026161\n",
      "0.0593 --- loss: 0.029147\n",
      "0.0600 --- loss: 0.027408\n",
      "0.0607 --- loss: 0.029147\n",
      "0.0614 --- loss: 0.023734\n",
      "0.0621 --- loss: 0.030268\n",
      "0.0629 --- loss: 0.021975\n",
      "0.0636 --- loss: 0.028333\n",
      "0.0643 --- loss: 0.029432\n",
      "0.0650 --- loss: 0.029456\n",
      "0.0657 --- loss: 0.030061\n",
      "0.0664 --- loss: 0.023243\n",
      "0.0671 --- loss: 0.025546\n",
      "0.0679 --- loss: 0.028816\n",
      "0.0686 --- loss: 0.025450\n",
      "0.0693 --- loss: 0.027470\n",
      "0.0700 --- loss: 0.024086\n",
      "0.0707 --- loss: 0.026046\n",
      "0.0714 --- loss: 0.026834\n",
      "0.0721 --- loss: 0.028201\n",
      "0.0729 --- loss: 0.025167\n",
      "0.0736 --- loss: 0.028903\n",
      "0.0743 --- loss: 0.032507\n",
      "0.0750 --- loss: 0.019081\n",
      "0.0757 --- loss: 0.027527\n",
      "0.0764 --- loss: 0.023477\n",
      "0.0771 --- loss: 0.030150\n",
      "0.0779 --- loss: 0.028005\n",
      "0.0786 --- loss: 0.023446\n",
      "0.0793 --- loss: 0.026483\n",
      "0.0800 --- loss: 0.026400\n",
      "0.0807 --- loss: 0.029453\n",
      "0.0814 --- loss: 0.022530\n",
      "0.0821 --- loss: 0.031542\n",
      "0.0829 --- loss: 0.024153\n",
      "0.0836 --- loss: 0.022291\n",
      "0.0843 --- loss: 0.031276\n",
      "0.0850 --- loss: 0.022615\n",
      "0.0857 --- loss: 0.022948\n",
      "0.0864 --- loss: 0.030687\n",
      "0.0871 --- loss: 0.023484\n",
      "0.0879 --- loss: 0.027607\n",
      "0.0886 --- loss: 0.024392\n",
      "0.0893 --- loss: 0.023236\n",
      "0.0900 --- loss: 0.022150\n",
      "0.0907 --- loss: 0.028580\n",
      "0.0914 --- loss: 0.027159\n",
      "0.0921 --- loss: 0.025074\n",
      "0.0929 --- loss: 0.017704\n",
      "0.0936 --- loss: 0.022207\n",
      "0.0943 --- loss: 0.024293\n",
      "0.0950 --- loss: 0.026706\n",
      "0.0957 --- loss: 0.022611\n",
      "0.0964 --- loss: 0.017772\n",
      "0.0971 --- loss: 0.023441\n",
      "0.0979 --- loss: 0.022848\n",
      "0.0986 --- loss: 0.027158\n",
      "0.0993 --- loss: 0.020667\n",
      "0.1000 --- loss: 0.024963\n",
      "0.1007 --- loss: 0.018928\n",
      "0.1014 --- loss: 0.020659\n",
      "0.1021 --- loss: 0.024059\n",
      "0.1029 --- loss: 0.022957\n",
      "0.1036 --- loss: 0.022926\n",
      "0.1043 --- loss: 0.033290\n",
      "0.1050 --- loss: 0.026181\n",
      "0.1057 --- loss: 0.023365\n",
      "0.1064 --- loss: 0.027893\n",
      "0.1071 --- loss: 0.022240\n",
      "0.1079 --- loss: 0.023671\n",
      "0.1086 --- loss: 0.023868\n",
      "0.1093 --- loss: 0.022182\n",
      "0.1100 --- loss: 0.019028\n",
      "0.1107 --- loss: 0.031076\n",
      "0.1114 --- loss: 0.021038\n",
      "0.1121 --- loss: 0.025505\n",
      "0.1129 --- loss: 0.023191\n",
      "0.1136 --- loss: 0.020037\n",
      "0.1143 --- loss: 0.019152\n",
      "0.1150 --- loss: 0.024878\n",
      "0.1157 --- loss: 0.019765\n",
      "0.1164 --- loss: 0.020798\n",
      "0.1171 --- loss: 0.031409\n",
      "0.1179 --- loss: 0.022112\n",
      "0.1186 --- loss: 0.021740\n",
      "0.1193 --- loss: 0.019055\n",
      "0.1200 --- loss: 0.019649\n",
      "0.1207 --- loss: 0.021016\n",
      "0.1214 --- loss: 0.023020\n",
      "0.1221 --- loss: 0.024284\n",
      "0.1229 --- loss: 0.024254\n",
      "0.1236 --- loss: 0.021147\n",
      "0.1243 --- loss: 0.024453\n",
      "0.1250 --- loss: 0.022734\n",
      "0.1257 --- loss: 0.019918\n",
      "0.1264 --- loss: 0.019776\n",
      "0.1271 --- loss: 0.030393\n",
      "0.1279 --- loss: 0.024788\n",
      "0.1286 --- loss: 0.019251\n",
      "0.1293 --- loss: 0.022476\n",
      "0.1300 --- loss: 0.028046\n",
      "0.1307 --- loss: 0.026014\n",
      "0.1314 --- loss: 0.014131\n",
      "0.1321 --- loss: 0.022435\n",
      "0.1329 --- loss: 0.025497\n",
      "0.1336 --- loss: 0.022497\n",
      "0.1343 --- loss: 0.021484\n",
      "0.1350 --- loss: 0.024829\n",
      "0.1357 --- loss: 0.024586\n",
      "0.1364 --- loss: 0.017924\n",
      "0.1371 --- loss: 0.022488\n",
      "0.1379 --- loss: 0.024648\n",
      "0.1386 --- loss: 0.022050\n",
      "0.1393 --- loss: 0.021035\n",
      "0.1400 --- loss: 0.021481\n",
      "0.1407 --- loss: 0.027650\n",
      "0.1414 --- loss: 0.015208\n",
      "0.1421 --- loss: 0.021475\n",
      "0.1429 --- loss: 0.018199\n",
      "0.1436 --- loss: 0.019429\n",
      "0.1443 --- loss: 0.016889\n",
      "0.1450 --- loss: 0.022679\n",
      "0.1457 --- loss: 0.017781\n",
      "0.1464 --- loss: 0.022385\n",
      "0.1471 --- loss: 0.021941\n",
      "0.1479 --- loss: 0.018690\n",
      "0.1486 --- loss: 0.026023\n",
      "0.1493 --- loss: 0.018865\n",
      "0.1500 --- loss: 0.022147\n",
      "0.1507 --- loss: 0.028966\n",
      "0.1514 --- loss: 0.025054\n",
      "0.1521 --- loss: 0.018216\n",
      "0.1529 --- loss: 0.024144\n",
      "0.1536 --- loss: 0.019408\n",
      "0.1543 --- loss: 0.026566\n",
      "0.1550 --- loss: 0.017716\n",
      "0.1557 --- loss: 0.018586\n",
      "0.1564 --- loss: 0.019817\n",
      "0.1571 --- loss: 0.018992\n",
      "0.1579 --- loss: 0.021443\n",
      "0.1586 --- loss: 0.019014\n",
      "0.1593 --- loss: 0.017110\n",
      "0.1600 --- loss: 0.015372\n",
      "0.1607 --- loss: 0.019664\n",
      "0.1614 --- loss: 0.017769\n",
      "0.1621 --- loss: 0.023365\n",
      "0.1629 --- loss: 0.020686\n",
      "0.1636 --- loss: 0.025970\n",
      "0.1643 --- loss: 0.017691\n",
      "0.1650 --- loss: 0.022920\n",
      "0.1657 --- loss: 0.016076\n",
      "0.1664 --- loss: 0.027432\n",
      "0.1671 --- loss: 0.022236\n",
      "0.1679 --- loss: 0.022893\n",
      "0.1686 --- loss: 0.020318\n",
      "0.1693 --- loss: 0.017523\n",
      "0.1700 --- loss: 0.023288\n",
      "0.1707 --- loss: 0.021293\n",
      "0.1714 --- loss: 0.016724\n",
      "0.1721 --- loss: 0.034669\n",
      "0.1729 --- loss: 0.016902\n",
      "0.1736 --- loss: 0.016312\n",
      "0.1743 --- loss: 0.021896\n",
      "0.1750 --- loss: 0.017490\n",
      "0.1757 --- loss: 0.012372\n",
      "0.1764 --- loss: 0.021997\n",
      "0.1771 --- loss: 0.022456\n",
      "0.1779 --- loss: 0.018005\n",
      "0.1786 --- loss: 0.019322\n",
      "0.1793 --- loss: 0.023882\n",
      "0.1800 --- loss: 0.016935\n",
      "0.1807 --- loss: 0.024866\n",
      "0.1814 --- loss: 0.025141\n",
      "0.1821 --- loss: 0.016210\n",
      "0.1829 --- loss: 0.018787\n",
      "0.1836 --- loss: 0.018442\n",
      "0.1843 --- loss: 0.018553\n",
      "0.1850 --- loss: 0.021912\n",
      "0.1857 --- loss: 0.015115\n",
      "0.1864 --- loss: 0.016103\n",
      "0.1871 --- loss: 0.019805\n",
      "0.1879 --- loss: 0.022309\n",
      "0.1886 --- loss: 0.020072\n",
      "0.1893 --- loss: 0.021169\n",
      "0.1900 --- loss: 0.021731\n",
      "0.1907 --- loss: 0.021967\n",
      "0.1914 --- loss: 0.019914\n",
      "0.1921 --- loss: 0.022149\n",
      "0.1929 --- loss: 0.015263\n",
      "0.1936 --- loss: 0.019126\n",
      "0.1943 --- loss: 0.022686\n",
      "0.1950 --- loss: 0.021635\n",
      "0.1957 --- loss: 0.016164\n",
      "0.1964 --- loss: 0.018221\n",
      "0.1971 --- loss: 0.012858\n",
      "0.1979 --- loss: 0.016416\n",
      "0.1986 --- loss: 0.023829\n",
      "0.1993 --- loss: 0.017981\n",
      "0.2000 --- loss: 0.017803\n",
      "0.2007 --- loss: 0.019730\n",
      "0.2014 --- loss: 0.016872\n",
      "0.2021 --- loss: 0.015868\n",
      "0.2029 --- loss: 0.020871\n",
      "0.2036 --- loss: 0.013281\n",
      "0.2043 --- loss: 0.024867\n",
      "0.2050 --- loss: 0.019472\n",
      "0.2057 --- loss: 0.021858\n",
      "0.2064 --- loss: 0.017797\n",
      "0.2071 --- loss: 0.020584\n",
      "0.2079 --- loss: 0.016418\n",
      "0.2086 --- loss: 0.016922\n",
      "0.2093 --- loss: 0.018216\n",
      "0.2100 --- loss: 0.014420\n",
      "0.2107 --- loss: 0.020560\n",
      "0.2114 --- loss: 0.016746\n",
      "0.2121 --- loss: 0.014111\n",
      "0.2129 --- loss: 0.017719\n",
      "0.2136 --- loss: 0.019472\n",
      "0.2143 --- loss: 0.015406\n",
      "0.2150 --- loss: 0.013244\n",
      "0.2157 --- loss: 0.019477\n",
      "0.2164 --- loss: 0.015567\n",
      "0.2171 --- loss: 0.022171\n",
      "0.2179 --- loss: 0.014202\n",
      "0.2186 --- loss: 0.014962\n",
      "0.2193 --- loss: 0.016131\n",
      "0.2200 --- loss: 0.025522\n",
      "0.2207 --- loss: 0.016998\n",
      "0.2214 --- loss: 0.014406\n",
      "0.2221 --- loss: 0.019259\n",
      "0.2229 --- loss: 0.016763\n",
      "0.2236 --- loss: 0.015669\n",
      "0.2243 --- loss: 0.016220\n",
      "0.2250 --- loss: 0.018222\n",
      "0.2257 --- loss: 0.025486\n",
      "0.2264 --- loss: 0.018263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2271 --- loss: 0.013069\n",
      "0.2279 --- loss: 0.018558\n",
      "0.2286 --- loss: 0.016249\n",
      "0.2293 --- loss: 0.012628\n",
      "0.2300 --- loss: 0.010914\n",
      "0.2307 --- loss: 0.025765\n",
      "0.2314 --- loss: 0.017695\n",
      "0.2321 --- loss: 0.016150\n",
      "0.2329 --- loss: 0.021420\n",
      "0.2336 --- loss: 0.014876\n",
      "0.2343 --- loss: 0.010901\n",
      "0.2350 --- loss: 0.021344\n",
      "0.2357 --- loss: 0.021336\n",
      "0.2364 --- loss: 0.017361\n",
      "0.2371 --- loss: 0.020170\n",
      "0.2379 --- loss: 0.011378\n",
      "0.2386 --- loss: 0.017135\n",
      "0.2393 --- loss: 0.016247\n",
      "0.2400 --- loss: 0.013807\n",
      "0.2407 --- loss: 0.011877\n",
      "0.2414 --- loss: 0.015453\n",
      "0.2421 --- loss: 0.020374\n",
      "0.2429 --- loss: 0.020810\n",
      "0.2436 --- loss: 0.017134\n",
      "0.2443 --- loss: 0.011687\n",
      "0.2450 --- loss: 0.021007\n",
      "0.2457 --- loss: 0.015654\n",
      "0.2464 --- loss: 0.014932\n",
      "0.2471 --- loss: 0.016282\n",
      "0.2479 --- loss: 0.015517\n",
      "0.2486 --- loss: 0.014166\n",
      "0.2493 --- loss: 0.015696\n",
      "0.2500 --- loss: 0.019260\n",
      "0.2507 --- loss: 0.017511\n",
      "0.2514 --- loss: 0.014545\n",
      "0.2521 --- loss: 0.016204\n",
      "0.2529 --- loss: 0.014081\n",
      "0.2536 --- loss: 0.021576\n",
      "0.2543 --- loss: 0.018092\n",
      "0.2550 --- loss: 0.022788\n",
      "0.2557 --- loss: 0.016841\n",
      "0.2564 --- loss: 0.014538\n",
      "0.2571 --- loss: 0.016033\n",
      "0.2579 --- loss: 0.017505\n",
      "0.2586 --- loss: 0.013309\n",
      "0.2593 --- loss: 0.012688\n",
      "0.2600 --- loss: 0.015666\n",
      "0.2607 --- loss: 0.012566\n",
      "0.2614 --- loss: 0.016133\n",
      "0.2621 --- loss: 0.013403\n",
      "0.2629 --- loss: 0.019277\n",
      "0.2636 --- loss: 0.014404\n",
      "0.2643 --- loss: 0.013773\n",
      "0.2650 --- loss: 0.015214\n",
      "0.2657 --- loss: 0.014946\n",
      "0.2664 --- loss: 0.018226\n",
      "0.2671 --- loss: 0.017743\n",
      "0.2679 --- loss: 0.017416\n",
      "0.2686 --- loss: 0.014834\n",
      "0.2693 --- loss: 0.016818\n",
      "0.2700 --- loss: 0.014709\n",
      "0.2707 --- loss: 0.020452\n",
      "0.2714 --- loss: 0.013705\n",
      "0.2721 --- loss: 0.019389\n",
      "0.2729 --- loss: 0.012494\n",
      "0.2736 --- loss: 0.012833\n",
      "0.2743 --- loss: 0.021097\n",
      "0.2750 --- loss: 0.018588\n",
      "0.2757 --- loss: 0.014160\n",
      "0.2764 --- loss: 0.011440\n",
      "0.2771 --- loss: 0.019785\n",
      "0.2779 --- loss: 0.014287\n",
      "0.2786 --- loss: 0.013854\n",
      "0.2793 --- loss: 0.011747\n",
      "0.2800 --- loss: 0.013154\n",
      "0.2807 --- loss: 0.017448\n",
      "0.2814 --- loss: 0.018945\n",
      "0.2821 --- loss: 0.014838\n",
      "0.2829 --- loss: 0.018867\n",
      "0.2836 --- loss: 0.013975\n",
      "0.2843 --- loss: 0.019108\n",
      "0.2850 --- loss: 0.018391\n",
      "0.2857 --- loss: 0.012450\n",
      "0.2864 --- loss: 0.015329\n",
      "0.2871 --- loss: 0.014122\n",
      "0.2879 --- loss: 0.015935\n",
      "0.2886 --- loss: 0.018908\n",
      "0.2893 --- loss: 0.016122\n",
      "0.2900 --- loss: 0.018884\n",
      "0.2907 --- loss: 0.013851\n",
      "0.2914 --- loss: 0.013395\n",
      "0.2921 --- loss: 0.016040\n",
      "0.2929 --- loss: 0.012797\n",
      "0.2936 --- loss: 0.019097\n",
      "0.2943 --- loss: 0.014924\n",
      "0.2950 --- loss: 0.016852\n",
      "0.2957 --- loss: 0.015719\n",
      "0.2964 --- loss: 0.012280\n",
      "0.2971 --- loss: 0.011165\n",
      "0.2979 --- loss: 0.020739\n",
      "0.2986 --- loss: 0.013837\n",
      "0.2993 --- loss: 0.011606\n",
      "0.3000 --- loss: 0.015060\n",
      "0.3007 --- loss: 0.013496\n",
      "0.3014 --- loss: 0.013544\n",
      "0.3021 --- loss: 0.010775\n",
      "0.3029 --- loss: 0.015248\n",
      "0.3036 --- loss: 0.012900\n",
      "0.3043 --- loss: 0.019111\n",
      "0.3050 --- loss: 0.010541\n",
      "0.3057 --- loss: 0.013321\n",
      "0.3064 --- loss: 0.013397\n",
      "0.3071 --- loss: 0.015532\n",
      "0.3079 --- loss: 0.013045\n",
      "0.3086 --- loss: 0.018519\n",
      "0.3093 --- loss: 0.021932\n",
      "0.3100 --- loss: 0.017325\n",
      "0.3107 --- loss: 0.015132\n",
      "0.3114 --- loss: 0.017241\n",
      "0.3121 --- loss: 0.011333\n",
      "0.3129 --- loss: 0.014041\n",
      "0.3136 --- loss: 0.013910\n",
      "0.3143 --- loss: 0.015573\n",
      "0.3150 --- loss: 0.017899\n",
      "0.3157 --- loss: 0.013760\n",
      "0.3164 --- loss: 0.012350\n",
      "0.3171 --- loss: 0.011530\n",
      "0.3179 --- loss: 0.018935\n",
      "0.3186 --- loss: 0.015979\n",
      "0.3193 --- loss: 0.009753\n",
      "0.3200 --- loss: 0.014209\n",
      "0.3207 --- loss: 0.007937\n",
      "0.3214 --- loss: 0.012697\n",
      "0.3221 --- loss: 0.014646\n",
      "0.3229 --- loss: 0.012860\n",
      "0.3236 --- loss: 0.012414\n",
      "0.3243 --- loss: 0.017740\n",
      "0.3250 --- loss: 0.011557\n",
      "0.3257 --- loss: 0.012193\n",
      "0.3264 --- loss: 0.012826\n",
      "0.3271 --- loss: 0.017746\n",
      "0.3279 --- loss: 0.012504\n",
      "0.3286 --- loss: 0.013057\n",
      "0.3293 --- loss: 0.015940\n",
      "0.3300 --- loss: 0.013084\n",
      "0.3307 --- loss: 0.017547\n",
      "0.3314 --- loss: 0.014251\n",
      "0.3321 --- loss: 0.012740\n",
      "0.3329 --- loss: 0.017211\n",
      "0.3336 --- loss: 0.020433\n",
      "0.3343 --- loss: 0.021175\n",
      "0.3350 --- loss: 0.012613\n",
      "0.3357 --- loss: 0.014321\n",
      "0.3364 --- loss: 0.011015\n",
      "0.3371 --- loss: 0.014305\n",
      "0.3379 --- loss: 0.011158\n",
      "0.3386 --- loss: 0.017420\n",
      "0.3393 --- loss: 0.014464\n",
      "0.3400 --- loss: 0.015511\n",
      "0.3407 --- loss: 0.021797\n",
      "0.3414 --- loss: 0.013099\n",
      "0.3421 --- loss: 0.018062\n",
      "0.3429 --- loss: 0.015645\n",
      "0.3436 --- loss: 0.012691\n",
      "0.3443 --- loss: 0.010654\n",
      "0.3450 --- loss: 0.014199\n",
      "0.3457 --- loss: 0.018358\n",
      "0.3464 --- loss: 0.017749\n",
      "0.3471 --- loss: 0.010003\n",
      "0.3479 --- loss: 0.015508\n",
      "0.3486 --- loss: 0.011607\n",
      "0.3493 --- loss: 0.008620\n",
      "0.3500 --- loss: 0.012964\n",
      "0.3507 --- loss: 0.014111\n",
      "0.3514 --- loss: 0.014158\n",
      "0.3521 --- loss: 0.013919\n",
      "0.3529 --- loss: 0.015130\n",
      "0.3536 --- loss: 0.010164\n",
      "0.3543 --- loss: 0.014410\n",
      "0.3550 --- loss: 0.015153\n",
      "0.3557 --- loss: 0.013414\n",
      "0.3564 --- loss: 0.020069\n",
      "0.3571 --- loss: 0.012224\n",
      "0.3579 --- loss: 0.015160\n",
      "0.3586 --- loss: 0.017731\n",
      "0.3593 --- loss: 0.017242\n",
      "0.3600 --- loss: 0.012802\n",
      "0.3607 --- loss: 0.011139\n",
      "0.3614 --- loss: 0.011630\n",
      "0.3621 --- loss: 0.018393\n",
      "0.3629 --- loss: 0.010949\n",
      "0.3636 --- loss: 0.013626\n",
      "0.3643 --- loss: 0.014387\n",
      "0.3650 --- loss: 0.016945\n",
      "0.3657 --- loss: 0.018016\n",
      "0.3664 --- loss: 0.015134\n",
      "0.3671 --- loss: 0.013518\n",
      "0.3679 --- loss: 0.014320\n",
      "0.3686 --- loss: 0.013074\n",
      "0.3693 --- loss: 0.020122\n",
      "0.3700 --- loss: 0.012565\n",
      "0.3707 --- loss: 0.012862\n",
      "0.3714 --- loss: 0.013721\n",
      "0.3721 --- loss: 0.011565\n",
      "0.3729 --- loss: 0.014007\n",
      "0.3736 --- loss: 0.016321\n",
      "0.3743 --- loss: 0.009645\n",
      "0.3750 --- loss: 0.013071\n",
      "0.3757 --- loss: 0.009813\n",
      "0.3764 --- loss: 0.012808\n",
      "0.3771 --- loss: 0.010701\n",
      "0.3779 --- loss: 0.013156\n",
      "0.3786 --- loss: 0.012873\n",
      "0.3793 --- loss: 0.014009\n",
      "0.3800 --- loss: 0.013418\n",
      "0.3807 --- loss: 0.010348\n",
      "0.3814 --- loss: 0.014880\n",
      "0.3821 --- loss: 0.012769\n",
      "0.3829 --- loss: 0.008768\n",
      "0.3836 --- loss: 0.010536\n",
      "0.3843 --- loss: 0.013085\n",
      "0.3850 --- loss: 0.016665\n",
      "0.3857 --- loss: 0.013080\n",
      "0.3864 --- loss: 0.021521\n",
      "0.3871 --- loss: 0.012060\n",
      "0.3879 --- loss: 0.011774\n",
      "0.3886 --- loss: 0.015291\n",
      "0.3893 --- loss: 0.014005\n",
      "0.3900 --- loss: 0.012225\n",
      "0.3907 --- loss: 0.011695\n",
      "0.3914 --- loss: 0.013933\n",
      "0.3921 --- loss: 0.012758\n",
      "0.3929 --- loss: 0.009501\n",
      "0.3936 --- loss: 0.011683\n",
      "0.3943 --- loss: 0.009493\n",
      "0.3950 --- loss: 0.012266\n",
      "0.3957 --- loss: 0.019561\n",
      "0.3964 --- loss: 0.011141\n",
      "0.3971 --- loss: 0.014025\n",
      "0.3979 --- loss: 0.011463\n",
      "0.3986 --- loss: 0.011819\n",
      "0.3993 --- loss: 0.017204\n",
      "0.4000 --- loss: 0.012510\n",
      "0.4007 --- loss: 0.013285\n",
      "0.4014 --- loss: 0.016239\n",
      "0.4021 --- loss: 0.012935\n",
      "0.4029 --- loss: 0.008749\n",
      "0.4036 --- loss: 0.012389\n",
      "0.4043 --- loss: 0.014332\n",
      "0.4050 --- loss: 0.015527\n",
      "0.4057 --- loss: 0.015707\n",
      "0.4064 --- loss: 0.008877\n",
      "0.4071 --- loss: 0.014811\n",
      "0.4079 --- loss: 0.011871\n",
      "0.4086 --- loss: 0.011429\n",
      "0.4093 --- loss: 0.011423\n",
      "0.4100 --- loss: 0.018752\n",
      "0.4107 --- loss: 0.012318\n",
      "0.4114 --- loss: 0.010472\n",
      "0.4121 --- loss: 0.020549\n",
      "0.4129 --- loss: 0.015525\n",
      "0.4136 --- loss: 0.011048\n",
      "0.4143 --- loss: 0.009364\n",
      "0.4150 --- loss: 0.012214\n",
      "0.4157 --- loss: 0.012671\n",
      "0.4164 --- loss: 0.011575\n",
      "0.4171 --- loss: 0.015165\n",
      "0.4179 --- loss: 0.009872\n",
      "0.4186 --- loss: 0.014415\n",
      "0.4193 --- loss: 0.014878\n",
      "0.4200 --- loss: 0.012576\n",
      "0.4207 --- loss: 0.010208\n",
      "0.4214 --- loss: 0.015578\n",
      "0.4221 --- loss: 0.012482\n",
      "0.4229 --- loss: 0.013883\n",
      "0.4236 --- loss: 0.008999\n",
      "0.4243 --- loss: 0.014379\n",
      "0.4250 --- loss: 0.010618\n",
      "0.4257 --- loss: 0.014355\n",
      "0.4264 --- loss: 0.011459\n",
      "0.4271 --- loss: 0.019146\n",
      "0.4279 --- loss: 0.014366\n",
      "0.4286 --- loss: 0.009217\n",
      "0.4293 --- loss: 0.010824\n",
      "0.4300 --- loss: 0.010865\n",
      "0.4307 --- loss: 0.011637\n",
      "0.4314 --- loss: 0.014365\n",
      "0.4321 --- loss: 0.012257\n",
      "0.4329 --- loss: 0.010630\n",
      "0.4336 --- loss: 0.009951\n",
      "0.4343 --- loss: 0.014867\n",
      "0.4350 --- loss: 0.015367\n",
      "0.4357 --- loss: 0.013419\n",
      "0.4364 --- loss: 0.010763\n",
      "0.4371 --- loss: 0.015494\n",
      "0.4379 --- loss: 0.015085\n",
      "0.4386 --- loss: 0.018910\n",
      "0.4393 --- loss: 0.014178\n",
      "0.4400 --- loss: 0.015316\n",
      "0.4407 --- loss: 0.017844\n",
      "0.4414 --- loss: 0.011768\n",
      "0.4421 --- loss: 0.011767\n",
      "0.4429 --- loss: 0.009658\n",
      "0.4436 --- loss: 0.012548\n",
      "0.4443 --- loss: 0.013329\n",
      "0.4450 --- loss: 0.009336\n",
      "0.4457 --- loss: 0.012763\n",
      "0.4464 --- loss: 0.012886\n",
      "0.4471 --- loss: 0.010330\n",
      "0.4479 --- loss: 0.015730\n",
      "0.4486 --- loss: 0.011442\n",
      "0.4493 --- loss: 0.012391\n",
      "0.4500 --- loss: 0.017576\n",
      "0.4507 --- loss: 0.012427\n",
      "0.4514 --- loss: 0.012882\n",
      "0.4521 --- loss: 0.011341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4529 --- loss: 0.021606\n",
      "0.4536 --- loss: 0.015375\n",
      "0.4543 --- loss: 0.013227\n",
      "0.4550 --- loss: 0.012718\n",
      "0.4557 --- loss: 0.010407\n",
      "0.4564 --- loss: 0.011168\n",
      "0.4571 --- loss: 0.015874\n",
      "0.4579 --- loss: 0.013846\n",
      "0.4586 --- loss: 0.017369\n",
      "0.4593 --- loss: 0.017269\n",
      "0.4600 --- loss: 0.018268\n",
      "0.4607 --- loss: 0.011895\n",
      "0.4614 --- loss: 0.012550\n",
      "0.4621 --- loss: 0.013808\n",
      "0.4629 --- loss: 0.012940\n",
      "0.4636 --- loss: 0.010149\n",
      "0.4643 --- loss: 0.010713\n",
      "0.4650 --- loss: 0.012135\n",
      "0.4657 --- loss: 0.010354\n",
      "0.4664 --- loss: 0.012448\n",
      "0.4671 --- loss: 0.011893\n",
      "0.4679 --- loss: 0.012324\n",
      "0.4686 --- loss: 0.008727\n",
      "0.4693 --- loss: 0.011143\n",
      "0.4700 --- loss: 0.012085\n",
      "0.4707 --- loss: 0.011743\n",
      "0.4714 --- loss: 0.018160\n",
      "0.4721 --- loss: 0.014547\n",
      "0.4729 --- loss: 0.014965\n",
      "0.4736 --- loss: 0.010421\n",
      "0.4743 --- loss: 0.005991\n",
      "0.4750 --- loss: 0.013193\n",
      "0.4757 --- loss: 0.009332\n",
      "0.4764 --- loss: 0.009918\n",
      "0.4771 --- loss: 0.014275\n",
      "0.4779 --- loss: 0.010147\n",
      "0.4786 --- loss: 0.014221\n",
      "0.4793 --- loss: 0.013049\n",
      "0.4800 --- loss: 0.011733\n",
      "0.4807 --- loss: 0.016265\n",
      "0.4814 --- loss: 0.008252\n",
      "0.4821 --- loss: 0.007889\n",
      "0.4829 --- loss: 0.013965\n",
      "0.4836 --- loss: 0.012990\n",
      "0.4843 --- loss: 0.012537\n",
      "0.4850 --- loss: 0.015563\n",
      "0.4857 --- loss: 0.012213\n",
      "0.4864 --- loss: 0.017418\n",
      "0.4871 --- loss: 0.010676\n",
      "0.4879 --- loss: 0.010529\n",
      "0.4886 --- loss: 0.013985\n",
      "0.4893 --- loss: 0.010826\n",
      "0.4900 --- loss: 0.013349\n",
      "0.4907 --- loss: 0.010825\n",
      "0.4914 --- loss: 0.011933\n",
      "0.4921 --- loss: 0.012130\n",
      "0.4929 --- loss: 0.009289\n",
      "0.4936 --- loss: 0.019440\n",
      "0.4943 --- loss: 0.013866\n",
      "0.4950 --- loss: 0.015900\n",
      "0.4957 --- loss: 0.009875\n",
      "0.4964 --- loss: 0.008928\n",
      "0.4971 --- loss: 0.011435\n",
      "0.4979 --- loss: 0.011785\n",
      "0.4986 --- loss: 0.015535\n",
      "0.4993 --- loss: 0.009197\n",
      "0.5000 --- loss: 0.012290\n",
      "0.5007 --- loss: 0.012005\n",
      "0.5014 --- loss: 0.013212\n",
      "0.5021 --- loss: 0.013036\n",
      "0.5029 --- loss: 0.009983\n",
      "0.5036 --- loss: 0.011303\n",
      "0.5043 --- loss: 0.009404\n",
      "0.5050 --- loss: 0.014027\n",
      "0.5057 --- loss: 0.008006\n",
      "0.5064 --- loss: 0.010415\n",
      "0.5071 --- loss: 0.014711\n",
      "0.5079 --- loss: 0.009610\n",
      "0.5086 --- loss: 0.013708\n",
      "0.5093 --- loss: 0.010454\n",
      "0.5100 --- loss: 0.011212\n",
      "0.5107 --- loss: 0.013022\n",
      "0.5114 --- loss: 0.014550\n",
      "0.5121 --- loss: 0.008941\n",
      "0.5129 --- loss: 0.013612\n",
      "0.5136 --- loss: 0.006335\n",
      "0.5143 --- loss: 0.010255\n",
      "0.5150 --- loss: 0.012860\n",
      "0.5157 --- loss: 0.013352\n",
      "0.5164 --- loss: 0.008628\n",
      "0.5171 --- loss: 0.012140\n",
      "0.5179 --- loss: 0.012550\n",
      "0.5186 --- loss: 0.008655\n",
      "0.5193 --- loss: 0.011292\n",
      "0.5200 --- loss: 0.009033\n",
      "0.5207 --- loss: 0.011334\n",
      "0.5214 --- loss: 0.008529\n",
      "0.5221 --- loss: 0.019596\n",
      "0.5229 --- loss: 0.016754\n",
      "0.5236 --- loss: 0.017361\n",
      "0.5243 --- loss: 0.009591\n",
      "0.5250 --- loss: 0.013198\n",
      "0.5257 --- loss: 0.014294\n",
      "0.5264 --- loss: 0.014201\n",
      "0.5271 --- loss: 0.015445\n",
      "0.5279 --- loss: 0.009172\n",
      "0.5286 --- loss: 0.008841\n",
      "0.5293 --- loss: 0.015252\n",
      "0.5300 --- loss: 0.011576\n",
      "0.5307 --- loss: 0.012812\n",
      "0.5314 --- loss: 0.015615\n",
      "0.5321 --- loss: 0.009352\n",
      "0.5329 --- loss: 0.014707\n",
      "0.5336 --- loss: 0.013147\n",
      "0.5343 --- loss: 0.010527\n",
      "0.5350 --- loss: 0.009968\n",
      "0.5357 --- loss: 0.017672\n",
      "0.5364 --- loss: 0.012823\n",
      "0.5371 --- loss: 0.014664\n",
      "0.5379 --- loss: 0.012032\n",
      "0.5386 --- loss: 0.013509\n",
      "0.5393 --- loss: 0.014615\n",
      "0.5400 --- loss: 0.014536\n",
      "0.5407 --- loss: 0.011393\n",
      "0.5414 --- loss: 0.015290\n",
      "0.5421 --- loss: 0.016939\n",
      "0.5429 --- loss: 0.021465\n",
      "0.5436 --- loss: 0.013965\n",
      "0.5443 --- loss: 0.014112\n",
      "0.5450 --- loss: 0.015599\n",
      "0.5457 --- loss: 0.011903\n",
      "0.5464 --- loss: 0.009624\n",
      "0.5471 --- loss: 0.011200\n",
      "0.5479 --- loss: 0.012952\n",
      "0.5486 --- loss: 0.012388\n",
      "0.5493 --- loss: 0.014868\n",
      "0.5500 --- loss: 0.008678\n",
      "0.5507 --- loss: 0.013897\n",
      "0.5514 --- loss: 0.010444\n",
      "0.5521 --- loss: 0.012776\n",
      "0.5529 --- loss: 0.012109\n",
      "0.5536 --- loss: 0.015400\n",
      "0.5543 --- loss: 0.009411\n",
      "0.5550 --- loss: 0.010655\n",
      "0.5557 --- loss: 0.012281\n",
      "0.5564 --- loss: 0.012977\n",
      "0.5571 --- loss: 0.011774\n",
      "0.5579 --- loss: 0.012489\n",
      "0.5586 --- loss: 0.012632\n",
      "0.5593 --- loss: 0.010312\n",
      "0.5600 --- loss: 0.015029\n",
      "0.5607 --- loss: 0.017738\n",
      "0.5614 --- loss: 0.010981\n",
      "0.5621 --- loss: 0.012239\n",
      "0.5629 --- loss: 0.009891\n",
      "0.5636 --- loss: 0.011758\n",
      "0.5643 --- loss: 0.013650\n",
      "0.5650 --- loss: 0.009824\n",
      "0.5657 --- loss: 0.015077\n",
      "0.5664 --- loss: 0.017090\n",
      "0.5671 --- loss: 0.016593\n",
      "0.5679 --- loss: 0.011662\n",
      "0.5686 --- loss: 0.015668\n",
      "0.5693 --- loss: 0.015007\n",
      "0.5700 --- loss: 0.013513\n",
      "0.5707 --- loss: 0.018414\n",
      "0.5714 --- loss: 0.016255\n",
      "0.5721 --- loss: 0.009125\n",
      "0.5729 --- loss: 0.009402\n",
      "0.5736 --- loss: 0.010271\n",
      "0.5743 --- loss: 0.012251\n",
      "0.5750 --- loss: 0.012219\n",
      "0.5757 --- loss: 0.013264\n",
      "0.5764 --- loss: 0.011579\n",
      "0.5771 --- loss: 0.015613\n",
      "0.5779 --- loss: 0.016666\n",
      "0.5786 --- loss: 0.013006\n",
      "0.5793 --- loss: 0.010012\n",
      "0.5800 --- loss: 0.014683\n",
      "0.5807 --- loss: 0.014599\n",
      "0.5814 --- loss: 0.011778\n",
      "0.5821 --- loss: 0.012154\n",
      "0.5829 --- loss: 0.017140\n",
      "0.5836 --- loss: 0.013109\n",
      "0.5843 --- loss: 0.012748\n",
      "0.5850 --- loss: 0.008512\n",
      "0.5857 --- loss: 0.009312\n",
      "0.5864 --- loss: 0.016494\n",
      "0.5871 --- loss: 0.014761\n",
      "0.5879 --- loss: 0.017239\n",
      "0.5886 --- loss: 0.017218\n",
      "0.5893 --- loss: 0.014040\n",
      "0.5900 --- loss: 0.010918\n",
      "0.5907 --- loss: 0.010036\n",
      "0.5914 --- loss: 0.008984\n",
      "0.5921 --- loss: 0.008254\n",
      "0.5929 --- loss: 0.012541\n",
      "0.5936 --- loss: 0.011256\n",
      "0.5943 --- loss: 0.012768\n",
      "0.5950 --- loss: 0.012481\n",
      "0.5957 --- loss: 0.010457\n",
      "0.5964 --- loss: 0.008397\n",
      "0.5971 --- loss: 0.010931\n",
      "0.5979 --- loss: 0.011587\n",
      "0.5986 --- loss: 0.014211\n",
      "0.5993 --- loss: 0.016197\n",
      "0.6000 --- loss: 0.010894\n",
      "0.6007 --- loss: 0.007119\n",
      "0.6014 --- loss: 0.013879\n",
      "0.6021 --- loss: 0.012617\n",
      "0.6029 --- loss: 0.009866\n",
      "0.6036 --- loss: 0.012343\n",
      "0.6043 --- loss: 0.010981\n",
      "0.6050 --- loss: 0.011946\n",
      "0.6057 --- loss: 0.010217\n",
      "0.6064 --- loss: 0.008941\n",
      "0.6071 --- loss: 0.012293\n",
      "0.6079 --- loss: 0.014314\n",
      "0.6086 --- loss: 0.020984\n",
      "0.6093 --- loss: 0.016051\n",
      "0.6100 --- loss: 0.020697\n",
      "0.6107 --- loss: 0.013892\n",
      "0.6114 --- loss: 0.014136\n",
      "0.6121 --- loss: 0.011915\n",
      "0.6129 --- loss: 0.008995\n",
      "0.6136 --- loss: 0.011859\n",
      "0.6143 --- loss: 0.013400\n",
      "0.6150 --- loss: 0.011324\n",
      "0.6157 --- loss: 0.014185\n",
      "0.6164 --- loss: 0.008925\n",
      "0.6171 --- loss: 0.011386\n",
      "0.6179 --- loss: 0.010093\n",
      "0.6186 --- loss: 0.013238\n",
      "0.6193 --- loss: 0.009713\n",
      "0.6200 --- loss: 0.007829\n",
      "0.6207 --- loss: 0.008476\n",
      "0.6214 --- loss: 0.009751\n",
      "0.6221 --- loss: 0.012018\n",
      "0.6229 --- loss: 0.011262\n",
      "0.6236 --- loss: 0.008294\n",
      "0.6243 --- loss: 0.008013\n",
      "0.6250 --- loss: 0.009471\n",
      "0.6257 --- loss: 0.008359\n",
      "0.6264 --- loss: 0.010727\n",
      "0.6271 --- loss: 0.010878\n",
      "0.6279 --- loss: 0.009620\n",
      "0.6286 --- loss: 0.012364\n",
      "0.6293 --- loss: 0.012963\n",
      "0.6300 --- loss: 0.010044\n",
      "0.6307 --- loss: 0.013158\n",
      "0.6314 --- loss: 0.010005\n",
      "0.6321 --- loss: 0.011564\n",
      "0.6329 --- loss: 0.012157\n",
      "0.6336 --- loss: 0.010810\n",
      "0.6343 --- loss: 0.013476\n",
      "0.6350 --- loss: 0.008315\n",
      "0.6357 --- loss: 0.010162\n",
      "0.6364 --- loss: 0.012600\n",
      "0.6371 --- loss: 0.011944\n",
      "0.6379 --- loss: 0.012312\n",
      "0.6386 --- loss: 0.011443\n",
      "0.6393 --- loss: 0.009263\n",
      "0.6400 --- loss: 0.012247\n",
      "0.6407 --- loss: 0.016403\n",
      "0.6414 --- loss: 0.015924\n",
      "0.6421 --- loss: 0.012503\n",
      "0.6429 --- loss: 0.009566\n",
      "0.6436 --- loss: 0.006438\n",
      "0.6443 --- loss: 0.015212\n",
      "0.6450 --- loss: 0.010810\n",
      "0.6457 --- loss: 0.011042\n",
      "0.6464 --- loss: 0.016571\n",
      "0.6471 --- loss: 0.009633\n",
      "0.6479 --- loss: 0.009974\n",
      "0.6486 --- loss: 0.017725\n",
      "0.6493 --- loss: 0.010694\n",
      "0.6500 --- loss: 0.009247\n",
      "0.6507 --- loss: 0.009408\n",
      "0.6514 --- loss: 0.009818\n",
      "0.6521 --- loss: 0.011683\n",
      "0.6529 --- loss: 0.014783\n",
      "0.6536 --- loss: 0.014888\n",
      "0.6543 --- loss: 0.008629\n",
      "0.6550 --- loss: 0.014107\n",
      "0.6557 --- loss: 0.012670\n",
      "0.6564 --- loss: 0.008088\n",
      "0.6571 --- loss: 0.009769\n",
      "0.6579 --- loss: 0.009290\n",
      "0.6586 --- loss: 0.009358\n",
      "0.6593 --- loss: 0.011769\n",
      "0.6600 --- loss: 0.011318\n",
      "0.6607 --- loss: 0.010192\n",
      "0.6614 --- loss: 0.012753\n",
      "0.6621 --- loss: 0.009553\n",
      "0.6629 --- loss: 0.016122\n",
      "0.6636 --- loss: 0.011887\n",
      "0.6643 --- loss: 0.008950\n",
      "0.6650 --- loss: 0.013250\n",
      "0.6657 --- loss: 0.017111\n",
      "0.6664 --- loss: 0.008138\n",
      "0.6671 --- loss: 0.007436\n",
      "0.6679 --- loss: 0.011247\n",
      "0.6686 --- loss: 0.008485\n",
      "0.6693 --- loss: 0.008890\n",
      "0.6700 --- loss: 0.011009\n",
      "0.6707 --- loss: 0.009452\n",
      "0.6714 --- loss: 0.010137\n",
      "0.6721 --- loss: 0.018279\n",
      "0.6729 --- loss: 0.012374\n",
      "0.6736 --- loss: 0.013058\n",
      "0.6743 --- loss: 0.010691\n",
      "0.6750 --- loss: 0.011623\n",
      "0.6757 --- loss: 0.009923\n",
      "0.6764 --- loss: 0.012197\n",
      "0.6771 --- loss: 0.015130\n",
      "0.6779 --- loss: 0.014923\n",
      "0.6786 --- loss: 0.008760\n",
      "0.6793 --- loss: 0.008141\n",
      "0.6800 --- loss: 0.008538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6807 --- loss: 0.013427\n",
      "0.6814 --- loss: 0.008496\n",
      "0.6821 --- loss: 0.011605\n",
      "0.6829 --- loss: 0.008951\n",
      "0.6836 --- loss: 0.010448\n",
      "0.6843 --- loss: 0.011772\n",
      "0.6850 --- loss: 0.011956\n",
      "0.6857 --- loss: 0.011783\n",
      "0.6864 --- loss: 0.014084\n",
      "0.6871 --- loss: 0.011556\n",
      "0.6879 --- loss: 0.007232\n",
      "0.6886 --- loss: 0.009069\n",
      "0.6893 --- loss: 0.017699\n",
      "0.6900 --- loss: 0.009648\n",
      "0.6907 --- loss: 0.013059\n",
      "0.6914 --- loss: 0.016276\n",
      "0.6921 --- loss: 0.009777\n",
      "0.6929 --- loss: 0.011988\n",
      "0.6936 --- loss: 0.013034\n",
      "0.6943 --- loss: 0.011248\n",
      "0.6950 --- loss: 0.010326\n",
      "0.6957 --- loss: 0.010043\n",
      "0.6964 --- loss: 0.009170\n",
      "0.6971 --- loss: 0.007835\n",
      "0.6979 --- loss: 0.007974\n",
      "0.6986 --- loss: 0.012588\n",
      "0.6993 --- loss: 0.013065\n",
      "0.7000 --- loss: 0.015539\n",
      "0.7007 --- loss: 0.010986\n",
      "0.7014 --- loss: 0.009868\n",
      "0.7021 --- loss: 0.014391\n",
      "0.7029 --- loss: 0.012165\n",
      "0.7036 --- loss: 0.012118\n",
      "0.7043 --- loss: 0.015381\n",
      "0.7050 --- loss: 0.012098\n",
      "0.7057 --- loss: 0.012895\n",
      "0.7064 --- loss: 0.011806\n",
      "0.7071 --- loss: 0.013697\n",
      "0.7079 --- loss: 0.009929\n",
      "0.7086 --- loss: 0.011972\n",
      "0.7093 --- loss: 0.015400\n",
      "0.7100 --- loss: 0.011382\n",
      "0.7107 --- loss: 0.015564\n",
      "0.7114 --- loss: 0.012306\n",
      "0.7121 --- loss: 0.013126\n",
      "0.7129 --- loss: 0.009030\n",
      "0.7136 --- loss: 0.010387\n",
      "0.7143 --- loss: 0.009373\n",
      "0.7150 --- loss: 0.013404\n",
      "0.7157 --- loss: 0.019340\n",
      "0.7164 --- loss: 0.011562\n",
      "0.7171 --- loss: 0.010812\n",
      "0.7179 --- loss: 0.008554\n",
      "0.7186 --- loss: 0.010689\n",
      "0.7193 --- loss: 0.010564\n",
      "0.7200 --- loss: 0.013376\n",
      "0.7207 --- loss: 0.012396\n",
      "0.7214 --- loss: 0.007377\n",
      "0.7221 --- loss: 0.010847\n",
      "0.7229 --- loss: 0.010006\n",
      "0.7236 --- loss: 0.010726\n",
      "0.7243 --- loss: 0.009782\n",
      "0.7250 --- loss: 0.011613\n",
      "0.7257 --- loss: 0.009869\n",
      "0.7264 --- loss: 0.012446\n",
      "0.7271 --- loss: 0.018763\n",
      "0.7279 --- loss: 0.010590\n",
      "0.7286 --- loss: 0.011058\n",
      "0.7293 --- loss: 0.010969\n",
      "0.7300 --- loss: 0.010343\n",
      "0.7307 --- loss: 0.014873\n",
      "0.7314 --- loss: 0.007930\n",
      "0.7321 --- loss: 0.009910\n",
      "0.7329 --- loss: 0.013235\n",
      "0.7336 --- loss: 0.010762\n",
      "0.7343 --- loss: 0.010389\n",
      "0.7350 --- loss: 0.008764\n",
      "0.7357 --- loss: 0.011462\n",
      "0.7364 --- loss: 0.012013\n",
      "0.7371 --- loss: 0.009839\n",
      "0.7379 --- loss: 0.014300\n",
      "0.7386 --- loss: 0.014322\n",
      "0.7393 --- loss: 0.007954\n",
      "0.7400 --- loss: 0.012900\n",
      "0.7407 --- loss: 0.010767\n",
      "0.7414 --- loss: 0.015059\n",
      "0.7421 --- loss: 0.010358\n",
      "0.7429 --- loss: 0.011142\n",
      "0.7436 --- loss: 0.010421\n",
      "0.7443 --- loss: 0.010884\n",
      "0.7450 --- loss: 0.009700\n",
      "0.7457 --- loss: 0.014225\n",
      "0.7464 --- loss: 0.007930\n",
      "0.7471 --- loss: 0.007543\n",
      "0.7479 --- loss: 0.009059\n",
      "0.7486 --- loss: 0.021020\n",
      "0.7493 --- loss: 0.009567\n",
      "0.7500 --- loss: 0.017983\n",
      "0.7507 --- loss: 0.009610\n",
      "0.7514 --- loss: 0.013847\n",
      "0.7521 --- loss: 0.009325\n",
      "0.7529 --- loss: 0.018095\n",
      "0.7536 --- loss: 0.009473\n",
      "0.7543 --- loss: 0.013338\n",
      "0.7550 --- loss: 0.012349\n",
      "0.7557 --- loss: 0.016781\n",
      "0.7564 --- loss: 0.013682\n",
      "0.7571 --- loss: 0.009672\n",
      "0.7579 --- loss: 0.010862\n",
      "0.7586 --- loss: 0.010528\n",
      "0.7593 --- loss: 0.011152\n",
      "0.7600 --- loss: 0.007588\n",
      "0.7607 --- loss: 0.013507\n",
      "0.7614 --- loss: 0.010916\n",
      "0.7621 --- loss: 0.006328\n",
      "0.7629 --- loss: 0.012298\n",
      "0.7636 --- loss: 0.013827\n",
      "0.7643 --- loss: 0.009169\n",
      "0.7650 --- loss: 0.009455\n",
      "0.7657 --- loss: 0.009963\n",
      "0.7664 --- loss: 0.012756\n",
      "0.7671 --- loss: 0.013399\n",
      "0.7679 --- loss: 0.012847\n",
      "0.7686 --- loss: 0.012822\n",
      "0.7693 --- loss: 0.009908\n",
      "0.7700 --- loss: 0.009121\n",
      "0.7707 --- loss: 0.010715\n",
      "0.7714 --- loss: 0.014004\n",
      "0.7721 --- loss: 0.012198\n",
      "0.7729 --- loss: 0.013769\n",
      "0.7736 --- loss: 0.011503\n",
      "0.7743 --- loss: 0.012573\n",
      "0.7750 --- loss: 0.012356\n",
      "0.7757 --- loss: 0.010556\n",
      "0.7764 --- loss: 0.010111\n",
      "0.7771 --- loss: 0.016611\n",
      "0.7779 --- loss: 0.014932\n",
      "0.7786 --- loss: 0.007691\n",
      "0.7793 --- loss: 0.015125\n",
      "0.7800 --- loss: 0.014646\n",
      "0.7807 --- loss: 0.011972\n",
      "0.7814 --- loss: 0.009836\n",
      "0.7821 --- loss: 0.012100\n",
      "0.7829 --- loss: 0.012044\n",
      "0.7836 --- loss: 0.009152\n",
      "0.7843 --- loss: 0.009724\n",
      "0.7850 --- loss: 0.010766\n",
      "0.7857 --- loss: 0.009692\n",
      "0.7864 --- loss: 0.011175\n",
      "0.7871 --- loss: 0.012794\n",
      "0.7879 --- loss: 0.013104\n",
      "0.7886 --- loss: 0.011281\n",
      "0.7893 --- loss: 0.011608\n",
      "0.7900 --- loss: 0.014889\n",
      "0.7907 --- loss: 0.009942\n",
      "0.7914 --- loss: 0.010930\n",
      "0.7921 --- loss: 0.012831\n",
      "0.7929 --- loss: 0.014020\n",
      "0.7936 --- loss: 0.009134\n",
      "0.7943 --- loss: 0.010944\n",
      "0.7950 --- loss: 0.014800\n",
      "0.7957 --- loss: 0.009178\n",
      "0.7964 --- loss: 0.009830\n",
      "0.7971 --- loss: 0.013309\n",
      "0.7979 --- loss: 0.013429\n",
      "0.7986 --- loss: 0.010458\n",
      "0.7993 --- loss: 0.009800\n",
      "0.8000 --- loss: 0.012066\n",
      "0.8007 --- loss: 0.018817\n",
      "0.8014 --- loss: 0.014695\n",
      "0.8021 --- loss: 0.012748\n",
      "0.8029 --- loss: 0.009468\n",
      "0.8036 --- loss: 0.012145\n",
      "0.8043 --- loss: 0.007483\n",
      "0.8050 --- loss: 0.011593\n",
      "0.8057 --- loss: 0.006191\n",
      "0.8064 --- loss: 0.013947\n",
      "0.8071 --- loss: 0.011373\n",
      "0.8079 --- loss: 0.010812\n",
      "0.8086 --- loss: 0.015797\n",
      "0.8093 --- loss: 0.010141\n",
      "0.8100 --- loss: 0.011719\n",
      "0.8107 --- loss: 0.010688\n",
      "0.8114 --- loss: 0.007442\n",
      "0.8121 --- loss: 0.010040\n",
      "0.8129 --- loss: 0.010647\n",
      "0.8136 --- loss: 0.014395\n",
      "0.8143 --- loss: 0.009458\n",
      "0.8150 --- loss: 0.007575\n",
      "0.8157 --- loss: 0.010295\n",
      "0.8164 --- loss: 0.008176\n",
      "0.8171 --- loss: 0.016764\n",
      "0.8179 --- loss: 0.007705\n",
      "0.8186 --- loss: 0.014594\n",
      "0.8193 --- loss: 0.014619\n",
      "0.8200 --- loss: 0.021495\n",
      "0.8207 --- loss: 0.008948\n",
      "0.8214 --- loss: 0.010418\n",
      "0.8221 --- loss: 0.011541\n",
      "0.8229 --- loss: 0.016775\n",
      "0.8236 --- loss: 0.015752\n",
      "0.8243 --- loss: 0.014656\n",
      "0.8250 --- loss: 0.013328\n",
      "0.8257 --- loss: 0.011857\n",
      "0.8264 --- loss: 0.010280\n",
      "0.8271 --- loss: 0.009072\n",
      "0.8279 --- loss: 0.009703\n",
      "0.8286 --- loss: 0.012865\n",
      "0.8293 --- loss: 0.012362\n",
      "0.8300 --- loss: 0.013454\n",
      "0.8307 --- loss: 0.007968\n",
      "0.8314 --- loss: 0.009271\n",
      "0.8321 --- loss: 0.008224\n",
      "0.8329 --- loss: 0.012087\n",
      "0.8336 --- loss: 0.010904\n",
      "0.8343 --- loss: 0.012759\n",
      "0.8350 --- loss: 0.010314\n",
      "0.8357 --- loss: 0.008020\n",
      "0.8364 --- loss: 0.007842\n",
      "0.8371 --- loss: 0.012688\n",
      "0.8379 --- loss: 0.009918\n",
      "0.8386 --- loss: 0.008944\n",
      "0.8393 --- loss: 0.006696\n",
      "0.8400 --- loss: 0.013452\n",
      "0.8407 --- loss: 0.007243\n",
      "0.8414 --- loss: 0.008719\n",
      "0.8421 --- loss: 0.013207\n",
      "0.8429 --- loss: 0.008980\n",
      "0.8436 --- loss: 0.011257\n",
      "0.8443 --- loss: 0.010807\n",
      "0.8450 --- loss: 0.007491\n",
      "0.8457 --- loss: 0.015942\n",
      "0.8464 --- loss: 0.013033\n",
      "0.8471 --- loss: 0.012769\n",
      "0.8479 --- loss: 0.008580\n",
      "0.8486 --- loss: 0.009187\n",
      "0.8493 --- loss: 0.013596\n",
      "0.8500 --- loss: 0.015491\n",
      "0.8507 --- loss: 0.021940\n",
      "0.8514 --- loss: 0.012506\n",
      "0.8521 --- loss: 0.012152\n",
      "0.8529 --- loss: 0.007921\n",
      "0.8536 --- loss: 0.008416\n",
      "0.8543 --- loss: 0.007143\n",
      "0.8550 --- loss: 0.009811\n",
      "0.8557 --- loss: 0.009159\n",
      "0.8564 --- loss: 0.010015\n",
      "0.8571 --- loss: 0.007494\n",
      "0.8579 --- loss: 0.010376\n",
      "0.8586 --- loss: 0.009249\n",
      "0.8593 --- loss: 0.009034\n",
      "0.8600 --- loss: 0.015725\n",
      "0.8607 --- loss: 0.011510\n",
      "0.8614 --- loss: 0.005834\n",
      "0.8621 --- loss: 0.009743\n",
      "0.8629 --- loss: 0.009010\n",
      "0.8636 --- loss: 0.011831\n",
      "0.8643 --- loss: 0.010596\n",
      "0.8650 --- loss: 0.011601\n",
      "0.8657 --- loss: 0.016166\n",
      "0.8664 --- loss: 0.010312\n",
      "0.8671 --- loss: 0.009963\n",
      "0.8679 --- loss: 0.008535\n",
      "0.8686 --- loss: 0.008480\n",
      "0.8693 --- loss: 0.015455\n",
      "0.8700 --- loss: 0.012278\n",
      "0.8707 --- loss: 0.011944\n",
      "0.8714 --- loss: 0.007388\n",
      "0.8721 --- loss: 0.008777\n",
      "0.8729 --- loss: 0.007820\n",
      "0.8736 --- loss: 0.010893\n",
      "0.8743 --- loss: 0.009504\n",
      "0.8750 --- loss: 0.008493\n",
      "0.8757 --- loss: 0.009526\n",
      "0.8764 --- loss: 0.013032\n",
      "0.8771 --- loss: 0.014444\n",
      "0.8779 --- loss: 0.014131\n",
      "0.8786 --- loss: 0.008837\n",
      "0.8793 --- loss: 0.009390\n",
      "0.8800 --- loss: 0.009739\n",
      "0.8807 --- loss: 0.010218\n",
      "0.8814 --- loss: 0.011586\n",
      "0.8821 --- loss: 0.013613\n",
      "0.8829 --- loss: 0.013464\n",
      "0.8836 --- loss: 0.008358\n",
      "0.8843 --- loss: 0.010440\n",
      "0.8850 --- loss: 0.013713\n",
      "0.8857 --- loss: 0.011203\n",
      "0.8864 --- loss: 0.009209\n",
      "0.8871 --- loss: 0.010382\n",
      "0.8879 --- loss: 0.007277\n",
      "0.8886 --- loss: 0.010346\n",
      "0.8893 --- loss: 0.013138\n",
      "0.8900 --- loss: 0.018914\n",
      "0.8907 --- loss: 0.011736\n",
      "0.8914 --- loss: 0.014143\n",
      "0.8921 --- loss: 0.009928\n",
      "0.8929 --- loss: 0.015237\n",
      "0.8936 --- loss: 0.006561\n",
      "0.8943 --- loss: 0.017764\n",
      "0.8950 --- loss: 0.014720\n",
      "0.8957 --- loss: 0.010040\n",
      "0.8964 --- loss: 0.008122\n",
      "0.8971 --- loss: 0.013963\n",
      "0.8979 --- loss: 0.008148\n",
      "0.8986 --- loss: 0.005920\n",
      "0.8993 --- loss: 0.015952\n",
      "0.9000 --- loss: 0.015241\n",
      "0.9007 --- loss: 0.012988\n",
      "0.9014 --- loss: 0.020541\n",
      "0.9021 --- loss: 0.009170\n",
      "0.9029 --- loss: 0.009948\n",
      "0.9036 --- loss: 0.012214\n",
      "0.9043 --- loss: 0.020547\n",
      "0.9050 --- loss: 0.008913\n",
      "0.9057 --- loss: 0.008305\n",
      "0.9064 --- loss: 0.011558\n",
      "0.9071 --- loss: 0.006533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9079 --- loss: 0.007774\n",
      "0.9086 --- loss: 0.012488\n",
      "0.9093 --- loss: 0.009959\n",
      "0.9100 --- loss: 0.010267\n",
      "0.9107 --- loss: 0.008843\n",
      "0.9114 --- loss: 0.011688\n",
      "0.9121 --- loss: 0.009275\n",
      "0.9129 --- loss: 0.009723\n",
      "0.9136 --- loss: 0.010683\n",
      "0.9143 --- loss: 0.010608\n",
      "0.9150 --- loss: 0.014164\n",
      "0.9157 --- loss: 0.009462\n",
      "0.9164 --- loss: 0.012381\n",
      "0.9171 --- loss: 0.008363\n",
      "0.9179 --- loss: 0.008561\n",
      "0.9186 --- loss: 0.007181\n",
      "0.9193 --- loss: 0.011392\n",
      "0.9200 --- loss: 0.012070\n",
      "0.9207 --- loss: 0.009685\n",
      "0.9214 --- loss: 0.010420\n",
      "0.9221 --- loss: 0.012324\n",
      "0.9229 --- loss: 0.010068\n",
      "0.9236 --- loss: 0.015346\n",
      "0.9243 --- loss: 0.012012\n",
      "0.9250 --- loss: 0.015131\n",
      "0.9257 --- loss: 0.013071\n",
      "0.9264 --- loss: 0.005803\n",
      "0.9271 --- loss: 0.012158\n",
      "0.9279 --- loss: 0.010629\n",
      "0.9286 --- loss: 0.011088\n",
      "0.9293 --- loss: 0.011349\n",
      "0.9300 --- loss: 0.009341\n",
      "0.9307 --- loss: 0.010952\n",
      "0.9314 --- loss: 0.016364\n",
      "0.9321 --- loss: 0.007490\n",
      "0.9329 --- loss: 0.014054\n",
      "0.9336 --- loss: 0.007962\n",
      "0.9343 --- loss: 0.011573\n",
      "0.9350 --- loss: 0.009268\n",
      "0.9357 --- loss: 0.012181\n",
      "0.9364 --- loss: 0.013078\n",
      "0.9371 --- loss: 0.011962\n",
      "0.9379 --- loss: 0.010502\n",
      "0.9386 --- loss: 0.008960\n",
      "0.9393 --- loss: 0.012765\n",
      "0.9400 --- loss: 0.013418\n",
      "0.9407 --- loss: 0.012627\n",
      "0.9414 --- loss: 0.019939\n",
      "0.9421 --- loss: 0.010981\n",
      "0.9429 --- loss: 0.011678\n",
      "0.9436 --- loss: 0.014913\n",
      "0.9443 --- loss: 0.008725\n",
      "0.9450 --- loss: 0.011709\n",
      "0.9457 --- loss: 0.016335\n",
      "0.9464 --- loss: 0.006889\n",
      "0.9471 --- loss: 0.012930\n",
      "0.9479 --- loss: 0.017557\n",
      "0.9486 --- loss: 0.012949\n",
      "0.9493 --- loss: 0.007795\n",
      "0.9500 --- loss: 0.014833\n",
      "0.9507 --- loss: 0.014131\n",
      "0.9514 --- loss: 0.008427\n",
      "0.9521 --- loss: 0.016669\n",
      "0.9529 --- loss: 0.009329\n",
      "0.9536 --- loss: 0.010876\n",
      "0.9543 --- loss: 0.009427\n",
      "0.9550 --- loss: 0.010946\n",
      "0.9557 --- loss: 0.012875\n",
      "0.9564 --- loss: 0.010747\n",
      "0.9571 --- loss: 0.010602\n",
      "0.9579 --- loss: 0.011735\n",
      "0.9586 --- loss: 0.013745\n",
      "0.9593 --- loss: 0.016131\n",
      "0.9600 --- loss: 0.006121\n",
      "0.9607 --- loss: 0.011257\n",
      "0.9614 --- loss: 0.007488\n",
      "0.9621 --- loss: 0.012482\n",
      "0.9629 --- loss: 0.012808\n",
      "0.9636 --- loss: 0.009612\n",
      "0.9643 --- loss: 0.011376\n",
      "0.9650 --- loss: 0.013409\n",
      "0.9657 --- loss: 0.010785\n",
      "0.9664 --- loss: 0.012011\n",
      "0.9671 --- loss: 0.011247\n",
      "0.9679 --- loss: 0.008858\n",
      "0.9686 --- loss: 0.016289\n",
      "0.9693 --- loss: 0.016560\n",
      "0.9700 --- loss: 0.016065\n",
      "0.9707 --- loss: 0.017365\n",
      "0.9714 --- loss: 0.014471\n",
      "0.9721 --- loss: 0.007055\n",
      "0.9729 --- loss: 0.006942\n",
      "0.9736 --- loss: 0.010784\n",
      "0.9743 --- loss: 0.010144\n",
      "0.9750 --- loss: 0.011771\n",
      "0.9757 --- loss: 0.013621\n",
      "0.9764 --- loss: 0.009562\n",
      "0.9771 --- loss: 0.011681\n",
      "0.9779 --- loss: 0.012040\n",
      "0.9786 --- loss: 0.011161\n",
      "0.9793 --- loss: 0.014029\n",
      "0.9800 --- loss: 0.013124\n",
      "0.9807 --- loss: 0.012374\n",
      "0.9814 --- loss: 0.012841\n",
      "0.9821 --- loss: 0.015920\n",
      "0.9829 --- loss: 0.008076\n",
      "0.9836 --- loss: 0.015956\n",
      "0.9843 --- loss: 0.011447\n",
      "0.9850 --- loss: 0.010155\n",
      "0.9857 --- loss: 0.010557\n",
      "0.9864 --- loss: 0.010224\n",
      "0.9871 --- loss: 0.012443\n",
      "0.9879 --- loss: 0.013621\n",
      "0.9886 --- loss: 0.008639\n",
      "0.9893 --- loss: 0.010525\n",
      "0.9900 --- loss: 0.012559\n",
      "0.9907 --- loss: 0.008375\n",
      "0.9914 --- loss: 0.005333\n",
      "0.9921 --- loss: 0.010089\n",
      "0.9929 --- loss: 0.010378\n",
      "0.9936 --- loss: 0.013829\n",
      "0.9943 --- loss: 0.008343\n",
      "0.9950 --- loss: 0.013881\n",
      "0.9957 --- loss: 0.014296\n",
      "0.9964 --- loss: 0.008073\n",
      "0.9971 --- loss: 0.010841\n",
      "0.9979 --- loss: 0.007902\n",
      "0.9986 --- loss: 0.014172\n",
      "0.9993 --- loss: 0.015670\n",
      "Epoch finished ! Loss: 0.01674009592114569\n",
      "Validation Dice Coeff: tensor([0.0105], grad_fn=<DivBackward0>)\n",
      "Checkpoint 1 saved !\n",
      "Starting epoch 2/2.\n",
      "0.0000 --- loss: 0.008478\n",
      "0.0007 --- loss: 0.014699\n",
      "0.0014 --- loss: 0.011327\n",
      "0.0021 --- loss: 0.008981\n",
      "0.0029 --- loss: 0.010070\n",
      "0.0036 --- loss: 0.008924\n",
      "0.0043 --- loss: 0.009330\n",
      "0.0050 --- loss: 0.008574\n",
      "0.0057 --- loss: 0.012123\n",
      "0.0064 --- loss: 0.010506\n",
      "0.0071 --- loss: 0.009507\n",
      "0.0079 --- loss: 0.008165\n",
      "0.0086 --- loss: 0.010207\n",
      "0.0093 --- loss: 0.009720\n",
      "0.0100 --- loss: 0.007694\n",
      "0.0107 --- loss: 0.011325\n",
      "0.0114 --- loss: 0.009558\n",
      "0.0121 --- loss: 0.008137\n",
      "0.0129 --- loss: 0.011680\n",
      "0.0136 --- loss: 0.006789\n",
      "0.0143 --- loss: 0.007963\n",
      "0.0150 --- loss: 0.008842\n",
      "0.0157 --- loss: 0.009643\n",
      "0.0164 --- loss: 0.013021\n",
      "0.0171 --- loss: 0.008800\n",
      "0.0179 --- loss: 0.007573\n",
      "0.0186 --- loss: 0.010758\n",
      "0.0193 --- loss: 0.008940\n",
      "0.0200 --- loss: 0.010521\n",
      "0.0207 --- loss: 0.007620\n",
      "0.0214 --- loss: 0.010735\n",
      "0.0221 --- loss: 0.010087\n",
      "0.0229 --- loss: 0.006869\n",
      "0.0236 --- loss: 0.007876\n",
      "0.0243 --- loss: 0.009417\n",
      "0.0250 --- loss: 0.013737\n",
      "0.0257 --- loss: 0.013672\n",
      "0.0264 --- loss: 0.011389\n",
      "0.0271 --- loss: 0.020031\n",
      "0.0279 --- loss: 0.011146\n",
      "0.0286 --- loss: 0.012872\n",
      "0.0293 --- loss: 0.006453\n",
      "0.0300 --- loss: 0.008633\n",
      "0.0307 --- loss: 0.005874\n",
      "0.0314 --- loss: 0.011786\n",
      "0.0321 --- loss: 0.009719\n",
      "0.0329 --- loss: 0.009150\n",
      "0.0336 --- loss: 0.012601\n",
      "0.0343 --- loss: 0.012625\n",
      "0.0350 --- loss: 0.011633\n",
      "0.0357 --- loss: 0.014596\n",
      "0.0364 --- loss: 0.011187\n",
      "0.0371 --- loss: 0.012869\n",
      "0.0379 --- loss: 0.010671\n",
      "0.0386 --- loss: 0.012062\n",
      "0.0393 --- loss: 0.007481\n",
      "0.0400 --- loss: 0.012586\n",
      "0.0407 --- loss: 0.011228\n",
      "0.0414 --- loss: 0.008801\n",
      "0.0421 --- loss: 0.008606\n",
      "0.0429 --- loss: 0.011432\n",
      "0.0436 --- loss: 0.011926\n",
      "0.0443 --- loss: 0.005182\n",
      "0.0450 --- loss: 0.009118\n",
      "0.0457 --- loss: 0.007765\n",
      "0.0464 --- loss: 0.009809\n",
      "0.0471 --- loss: 0.012586\n",
      "0.0479 --- loss: 0.012062\n",
      "0.0486 --- loss: 0.007405\n",
      "0.0493 --- loss: 0.012536\n",
      "0.0500 --- loss: 0.007045\n",
      "0.0507 --- loss: 0.009295\n",
      "0.0514 --- loss: 0.009241\n",
      "0.0521 --- loss: 0.007859\n",
      "0.0529 --- loss: 0.007423\n",
      "0.0536 --- loss: 0.010769\n",
      "0.0543 --- loss: 0.008417\n",
      "0.0550 --- loss: 0.008456\n",
      "0.0557 --- loss: 0.006876\n",
      "0.0564 --- loss: 0.009281\n",
      "0.0571 --- loss: 0.012053\n",
      "0.0579 --- loss: 0.008981\n",
      "0.0586 --- loss: 0.011636\n",
      "0.0593 --- loss: 0.009056\n",
      "0.0600 --- loss: 0.009838\n",
      "0.0607 --- loss: 0.007411\n",
      "0.0614 --- loss: 0.009947\n",
      "0.0621 --- loss: 0.005488\n",
      "0.0629 --- loss: 0.014170\n",
      "0.0636 --- loss: 0.008096\n",
      "0.0643 --- loss: 0.010194\n",
      "0.0650 --- loss: 0.011151\n",
      "0.0657 --- loss: 0.012936\n",
      "0.0664 --- loss: 0.010729\n",
      "0.0671 --- loss: 0.008010\n",
      "0.0679 --- loss: 0.009052\n",
      "0.0686 --- loss: 0.008208\n",
      "0.0693 --- loss: 0.008984\n",
      "0.0700 --- loss: 0.008643\n",
      "0.0707 --- loss: 0.009797\n",
      "0.0714 --- loss: 0.006086\n",
      "0.0721 --- loss: 0.007100\n",
      "0.0729 --- loss: 0.010427\n",
      "0.0736 --- loss: 0.010884\n",
      "0.0743 --- loss: 0.007324\n",
      "0.0750 --- loss: 0.008621\n",
      "0.0757 --- loss: 0.011583\n",
      "0.0764 --- loss: 0.006460\n",
      "0.0771 --- loss: 0.007895\n",
      "0.0779 --- loss: 0.015703\n",
      "0.0786 --- loss: 0.005802\n",
      "0.0793 --- loss: 0.007855\n",
      "0.0800 --- loss: 0.009399\n",
      "0.0807 --- loss: 0.010079\n",
      "0.0814 --- loss: 0.009486\n",
      "0.0821 --- loss: 0.014020\n",
      "0.0829 --- loss: 0.009062\n",
      "0.0836 --- loss: 0.011525\n",
      "0.0843 --- loss: 0.009451\n",
      "0.0850 --- loss: 0.008425\n",
      "0.0857 --- loss: 0.011262\n",
      "0.0864 --- loss: 0.009691\n",
      "0.0871 --- loss: 0.008113\n",
      "0.0879 --- loss: 0.007227\n",
      "0.0886 --- loss: 0.004957\n",
      "0.0893 --- loss: 0.012443\n",
      "0.0900 --- loss: 0.009549\n",
      "0.0907 --- loss: 0.008810\n",
      "0.0914 --- loss: 0.008558\n",
      "0.0921 --- loss: 0.011081\n",
      "0.0929 --- loss: 0.007841\n",
      "0.0936 --- loss: 0.010209\n",
      "0.0943 --- loss: 0.011033\n",
      "0.0950 --- loss: 0.005968\n",
      "0.0957 --- loss: 0.011241\n",
      "0.0964 --- loss: 0.012745\n",
      "0.0971 --- loss: 0.007678\n",
      "0.0979 --- loss: 0.011616\n",
      "0.0986 --- loss: 0.007585\n",
      "0.0993 --- loss: 0.015434\n",
      "0.1000 --- loss: 0.006857\n",
      "0.1007 --- loss: 0.008696\n",
      "0.1014 --- loss: 0.012093\n",
      "0.1021 --- loss: 0.008989\n",
      "0.1029 --- loss: 0.010562\n",
      "0.1036 --- loss: 0.011597\n",
      "0.1043 --- loss: 0.007872\n",
      "0.1050 --- loss: 0.017783\n",
      "0.1057 --- loss: 0.008822\n",
      "0.1064 --- loss: 0.010120\n",
      "0.1071 --- loss: 0.009812\n",
      "0.1079 --- loss: 0.007568\n",
      "0.1086 --- loss: 0.010509\n",
      "0.1093 --- loss: 0.007858\n",
      "0.1100 --- loss: 0.006306\n",
      "0.1107 --- loss: 0.010620\n",
      "0.1114 --- loss: 0.009788\n",
      "0.1121 --- loss: 0.011188\n",
      "0.1129 --- loss: 0.008275\n",
      "0.1136 --- loss: 0.011420\n",
      "0.1143 --- loss: 0.012638\n",
      "0.1150 --- loss: 0.012638\n",
      "0.1157 --- loss: 0.011809\n",
      "0.1164 --- loss: 0.012397\n",
      "0.1171 --- loss: 0.005089\n",
      "0.1179 --- loss: 0.009643\n",
      "0.1186 --- loss: 0.007108\n",
      "0.1193 --- loss: 0.012550\n",
      "0.1200 --- loss: 0.008252\n",
      "0.1207 --- loss: 0.012664\n",
      "0.1214 --- loss: 0.009469\n",
      "0.1221 --- loss: 0.007619\n",
      "0.1229 --- loss: 0.012009\n",
      "0.1236 --- loss: 0.007881\n",
      "0.1243 --- loss: 0.008336\n",
      "0.1250 --- loss: 0.009623\n",
      "0.1257 --- loss: 0.012552\n",
      "0.1264 --- loss: 0.008569\n",
      "0.1271 --- loss: 0.008310\n",
      "0.1279 --- loss: 0.008886\n",
      "0.1286 --- loss: 0.007043\n",
      "0.1293 --- loss: 0.007629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1300 --- loss: 0.007045\n",
      "0.1307 --- loss: 0.010619\n",
      "0.1314 --- loss: 0.012414\n",
      "0.1321 --- loss: 0.008103\n",
      "0.1329 --- loss: 0.008006\n",
      "0.1336 --- loss: 0.008811\n",
      "0.1343 --- loss: 0.005499\n",
      "0.1350 --- loss: 0.011610\n",
      "0.1357 --- loss: 0.009192\n",
      "0.1364 --- loss: 0.006602\n",
      "0.1371 --- loss: 0.012873\n",
      "0.1379 --- loss: 0.007834\n",
      "0.1386 --- loss: 0.012493\n",
      "0.1393 --- loss: 0.005966\n",
      "0.1400 --- loss: 0.011740\n",
      "0.1407 --- loss: 0.010907\n",
      "0.1414 --- loss: 0.008164\n",
      "0.1421 --- loss: 0.015617\n",
      "0.1429 --- loss: 0.007505\n",
      "0.1436 --- loss: 0.009362\n",
      "0.1443 --- loss: 0.012034\n",
      "0.1450 --- loss: 0.011412\n",
      "0.1457 --- loss: 0.007773\n",
      "0.1464 --- loss: 0.007584\n",
      "0.1471 --- loss: 0.009954\n",
      "0.1479 --- loss: 0.006764\n",
      "0.1486 --- loss: 0.008622\n",
      "0.1493 --- loss: 0.008121\n",
      "0.1500 --- loss: 0.009460\n",
      "0.1507 --- loss: 0.008899\n",
      "0.1514 --- loss: 0.009139\n",
      "0.1521 --- loss: 0.007706\n",
      "0.1529 --- loss: 0.007706\n",
      "0.1536 --- loss: 0.008796\n",
      "0.1543 --- loss: 0.006404\n",
      "0.1550 --- loss: 0.011038\n",
      "0.1557 --- loss: 0.012690\n",
      "0.1564 --- loss: 0.008046\n",
      "0.1571 --- loss: 0.007624\n",
      "0.1579 --- loss: 0.010604\n",
      "0.1586 --- loss: 0.009174\n",
      "0.1593 --- loss: 0.008504\n",
      "0.1600 --- loss: 0.007354\n",
      "0.1607 --- loss: 0.013298\n",
      "0.1614 --- loss: 0.008077\n",
      "0.1621 --- loss: 0.005839\n",
      "0.1629 --- loss: 0.009174\n",
      "0.1636 --- loss: 0.009471\n",
      "0.1643 --- loss: 0.013375\n",
      "0.1650 --- loss: 0.009938\n",
      "0.1657 --- loss: 0.010571\n",
      "0.1664 --- loss: 0.009914\n",
      "0.1671 --- loss: 0.008120\n",
      "0.1679 --- loss: 0.006466\n",
      "0.1686 --- loss: 0.005884\n",
      "0.1693 --- loss: 0.010073\n",
      "0.1700 --- loss: 0.008197\n",
      "0.1707 --- loss: 0.010972\n",
      "0.1714 --- loss: 0.010916\n",
      "0.1721 --- loss: 0.005519\n",
      "0.1729 --- loss: 0.011019\n",
      "0.1736 --- loss: 0.009560\n",
      "0.1743 --- loss: 0.010212\n",
      "0.1750 --- loss: 0.007032\n",
      "0.1757 --- loss: 0.011683\n",
      "0.1764 --- loss: 0.007701\n",
      "0.1771 --- loss: 0.009787\n",
      "0.1779 --- loss: 0.013128\n",
      "0.1786 --- loss: 0.007916\n",
      "0.1793 --- loss: 0.007211\n",
      "0.1800 --- loss: 0.005863\n",
      "0.1807 --- loss: 0.014507\n",
      "0.1814 --- loss: 0.009426\n",
      "0.1821 --- loss: 0.008881\n",
      "0.1829 --- loss: 0.006248\n",
      "0.1836 --- loss: 0.011916\n",
      "0.1843 --- loss: 0.009401\n",
      "0.1850 --- loss: 0.008150\n",
      "0.1857 --- loss: 0.007273\n",
      "0.1864 --- loss: 0.009278\n",
      "0.1871 --- loss: 0.009970\n",
      "0.1879 --- loss: 0.015768\n",
      "0.1886 --- loss: 0.010808\n",
      "0.1893 --- loss: 0.009061\n",
      "0.1900 --- loss: 0.009248\n",
      "0.1907 --- loss: 0.008777\n",
      "0.1914 --- loss: 0.007765\n",
      "0.1921 --- loss: 0.011669\n",
      "0.1929 --- loss: 0.006201\n",
      "0.1936 --- loss: 0.006483\n",
      "0.1943 --- loss: 0.013892\n",
      "0.1950 --- loss: 0.007687\n",
      "0.1957 --- loss: 0.010398\n",
      "0.1964 --- loss: 0.008084\n",
      "0.1971 --- loss: 0.007398\n",
      "0.1979 --- loss: 0.009369\n",
      "0.1986 --- loss: 0.009876\n",
      "0.1993 --- loss: 0.010406\n",
      "0.2000 --- loss: 0.011052\n",
      "0.2007 --- loss: 0.016822\n",
      "0.2014 --- loss: 0.010822\n",
      "0.2021 --- loss: 0.010693\n",
      "0.2029 --- loss: 0.011204\n",
      "0.2036 --- loss: 0.008290\n",
      "0.2043 --- loss: 0.008507\n",
      "0.2050 --- loss: 0.012192\n",
      "0.2057 --- loss: 0.012105\n",
      "0.2064 --- loss: 0.011687\n",
      "0.2071 --- loss: 0.009680\n",
      "0.2079 --- loss: 0.009255\n",
      "0.2086 --- loss: 0.008677\n",
      "0.2093 --- loss: 0.013165\n",
      "0.2100 --- loss: 0.009329\n",
      "0.2107 --- loss: 0.007569\n",
      "0.2114 --- loss: 0.012163\n",
      "0.2121 --- loss: 0.012286\n",
      "0.2129 --- loss: 0.008731\n",
      "0.2136 --- loss: 0.013666\n",
      "0.2143 --- loss: 0.011329\n",
      "0.2150 --- loss: 0.007689\n",
      "0.2157 --- loss: 0.011013\n",
      "0.2164 --- loss: 0.010762\n",
      "0.2171 --- loss: 0.009109\n",
      "0.2179 --- loss: 0.006765\n",
      "0.2186 --- loss: 0.009387\n",
      "0.2193 --- loss: 0.008532\n",
      "0.2200 --- loss: 0.005805\n",
      "0.2207 --- loss: 0.008049\n",
      "0.2214 --- loss: 0.007050\n",
      "0.2221 --- loss: 0.006486\n",
      "0.2229 --- loss: 0.010644\n",
      "0.2236 --- loss: 0.007310\n",
      "0.2243 --- loss: 0.008587\n",
      "0.2250 --- loss: 0.010572\n",
      "0.2257 --- loss: 0.009044\n",
      "0.2264 --- loss: 0.009745\n",
      "0.2271 --- loss: 0.010908\n",
      "0.2279 --- loss: 0.011881\n",
      "0.2286 --- loss: 0.010889\n",
      "0.2293 --- loss: 0.009425\n",
      "0.2300 --- loss: 0.010489\n",
      "0.2307 --- loss: 0.007803\n",
      "0.2314 --- loss: 0.008599\n",
      "0.2321 --- loss: 0.007005\n",
      "0.2329 --- loss: 0.008259\n",
      "0.2336 --- loss: 0.008969\n",
      "0.2343 --- loss: 0.007882\n",
      "0.2350 --- loss: 0.007934\n",
      "0.2357 --- loss: 0.015695\n",
      "0.2364 --- loss: 0.007736\n",
      "0.2371 --- loss: 0.011033\n",
      "0.2379 --- loss: 0.012128\n",
      "0.2386 --- loss: 0.009397\n",
      "0.2393 --- loss: 0.007509\n",
      "0.2400 --- loss: 0.007711\n",
      "0.2407 --- loss: 0.007350\n",
      "0.2414 --- loss: 0.007184\n",
      "0.2421 --- loss: 0.009926\n",
      "0.2429 --- loss: 0.010743\n",
      "0.2436 --- loss: 0.008557\n",
      "0.2443 --- loss: 0.007443\n",
      "0.2450 --- loss: 0.009302\n",
      "0.2457 --- loss: 0.010647\n",
      "0.2464 --- loss: 0.012576\n",
      "0.2471 --- loss: 0.006411\n",
      "0.2479 --- loss: 0.011190\n",
      "0.2486 --- loss: 0.008488\n",
      "0.2493 --- loss: 0.008476\n",
      "0.2500 --- loss: 0.009919\n",
      "0.2507 --- loss: 0.011657\n",
      "0.2514 --- loss: 0.008249\n",
      "0.2521 --- loss: 0.007793\n",
      "0.2529 --- loss: 0.015237\n",
      "0.2536 --- loss: 0.007801\n",
      "0.2543 --- loss: 0.009213\n",
      "0.2550 --- loss: 0.008047\n",
      "0.2557 --- loss: 0.007454\n",
      "0.2564 --- loss: 0.013236\n",
      "0.2571 --- loss: 0.010748\n",
      "0.2579 --- loss: 0.009694\n",
      "0.2586 --- loss: 0.007133\n",
      "0.2593 --- loss: 0.010468\n",
      "0.2600 --- loss: 0.016377\n",
      "0.2607 --- loss: 0.006660\n",
      "0.2614 --- loss: 0.010731\n",
      "0.2621 --- loss: 0.009975\n",
      "0.2629 --- loss: 0.006344\n",
      "0.2636 --- loss: 0.009273\n",
      "0.2643 --- loss: 0.010479\n",
      "0.2650 --- loss: 0.007680\n",
      "0.2657 --- loss: 0.007867\n",
      "0.2664 --- loss: 0.010752\n",
      "0.2671 --- loss: 0.010061\n",
      "0.2679 --- loss: 0.010213\n",
      "0.2686 --- loss: 0.010896\n",
      "0.2693 --- loss: 0.008691\n",
      "0.2700 --- loss: 0.007525\n",
      "0.2707 --- loss: 0.012670\n",
      "0.2714 --- loss: 0.007363\n",
      "0.2721 --- loss: 0.007404\n",
      "0.2729 --- loss: 0.012968\n",
      "0.2736 --- loss: 0.008427\n",
      "0.2743 --- loss: 0.008368\n",
      "0.2750 --- loss: 0.008589\n",
      "0.2757 --- loss: 0.010185\n",
      "0.2764 --- loss: 0.008860\n",
      "0.2771 --- loss: 0.009060\n",
      "0.2779 --- loss: 0.006878\n",
      "0.2786 --- loss: 0.007968\n",
      "0.2793 --- loss: 0.009488\n",
      "0.2800 --- loss: 0.006027\n",
      "0.2807 --- loss: 0.011872\n",
      "0.2814 --- loss: 0.005090\n",
      "0.2821 --- loss: 0.009638\n",
      "0.2829 --- loss: 0.009714\n",
      "0.2836 --- loss: 0.006703\n",
      "0.2843 --- loss: 0.007124\n",
      "0.2850 --- loss: 0.007635\n",
      "0.2857 --- loss: 0.007023\n",
      "0.2864 --- loss: 0.008217\n",
      "0.2871 --- loss: 0.005523\n",
      "0.2879 --- loss: 0.010115\n",
      "0.2886 --- loss: 0.005605\n",
      "0.2893 --- loss: 0.012064\n",
      "0.2900 --- loss: 0.011247\n",
      "0.2907 --- loss: 0.008039\n",
      "0.2914 --- loss: 0.007575\n",
      "0.2921 --- loss: 0.016773\n",
      "0.2929 --- loss: 0.006639\n",
      "0.2936 --- loss: 0.010330\n",
      "0.2943 --- loss: 0.007530\n",
      "0.2950 --- loss: 0.009092\n",
      "0.2957 --- loss: 0.009751\n",
      "0.2964 --- loss: 0.007907\n",
      "0.2971 --- loss: 0.010708\n",
      "0.2979 --- loss: 0.011596\n",
      "0.2986 --- loss: 0.007604\n",
      "0.2993 --- loss: 0.009696\n",
      "0.3000 --- loss: 0.011851\n",
      "0.3007 --- loss: 0.008701\n",
      "0.3014 --- loss: 0.008746\n",
      "0.3021 --- loss: 0.011113\n",
      "0.3029 --- loss: 0.009885\n",
      "0.3036 --- loss: 0.006319\n",
      "0.3043 --- loss: 0.006237\n",
      "0.3050 --- loss: 0.007852\n",
      "0.3057 --- loss: 0.009028\n",
      "0.3064 --- loss: 0.007564\n",
      "0.3071 --- loss: 0.010326\n",
      "0.3079 --- loss: 0.008272\n",
      "0.3086 --- loss: 0.011089\n",
      "0.3093 --- loss: 0.008547\n",
      "0.3100 --- loss: 0.007539\n",
      "0.3107 --- loss: 0.009960\n",
      "0.3114 --- loss: 0.009429\n",
      "0.3121 --- loss: 0.009103\n",
      "0.3129 --- loss: 0.009433\n",
      "0.3136 --- loss: 0.008538\n",
      "0.3143 --- loss: 0.009794\n",
      "0.3150 --- loss: 0.009325\n",
      "0.3157 --- loss: 0.007384\n",
      "0.3164 --- loss: 0.007701\n",
      "0.3171 --- loss: 0.009925\n",
      "0.3179 --- loss: 0.009653\n",
      "0.3186 --- loss: 0.012849\n",
      "0.3193 --- loss: 0.010006\n",
      "0.3200 --- loss: 0.009533\n",
      "0.3207 --- loss: 0.010045\n",
      "0.3214 --- loss: 0.010600\n",
      "0.3221 --- loss: 0.006931\n",
      "0.3229 --- loss: 0.007572\n",
      "0.3236 --- loss: 0.007185\n",
      "0.3243 --- loss: 0.005895\n",
      "0.3250 --- loss: 0.005958\n",
      "0.3257 --- loss: 0.010060\n",
      "0.3264 --- loss: 0.010985\n",
      "0.3271 --- loss: 0.006091\n",
      "0.3279 --- loss: 0.009600\n",
      "0.3286 --- loss: 0.008977\n",
      "0.3293 --- loss: 0.008548\n",
      "0.3300 --- loss: 0.009330\n",
      "0.3307 --- loss: 0.011564\n",
      "0.3314 --- loss: 0.009923\n",
      "0.3321 --- loss: 0.010273\n",
      "0.3329 --- loss: 0.009443\n",
      "0.3336 --- loss: 0.011180\n",
      "0.3343 --- loss: 0.011732\n",
      "0.3350 --- loss: 0.011236\n",
      "0.3357 --- loss: 0.010388\n",
      "0.3364 --- loss: 0.008639\n",
      "0.3371 --- loss: 0.009651\n",
      "0.3379 --- loss: 0.008895\n",
      "0.3386 --- loss: 0.008598\n",
      "0.3393 --- loss: 0.007704\n",
      "0.3400 --- loss: 0.006667\n",
      "0.3407 --- loss: 0.007457\n",
      "0.3414 --- loss: 0.011335\n",
      "0.3421 --- loss: 0.011753\n",
      "0.3429 --- loss: 0.013958\n",
      "0.3436 --- loss: 0.015426\n",
      "0.3443 --- loss: 0.012723\n",
      "0.3450 --- loss: 0.008732\n",
      "0.3457 --- loss: 0.008505\n",
      "0.3464 --- loss: 0.009623\n",
      "0.3471 --- loss: 0.010323\n",
      "0.3479 --- loss: 0.012368\n",
      "0.3486 --- loss: 0.005868\n",
      "0.3493 --- loss: 0.010195\n",
      "0.3500 --- loss: 0.007037\n",
      "0.3507 --- loss: 0.009320\n",
      "0.3514 --- loss: 0.006759\n",
      "0.3521 --- loss: 0.006989\n",
      "0.3529 --- loss: 0.008115\n",
      "0.3536 --- loss: 0.006015\n",
      "0.3543 --- loss: 0.006278\n",
      "0.3550 --- loss: 0.005301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3557 --- loss: 0.010310\n",
      "0.3564 --- loss: 0.008609\n",
      "0.3571 --- loss: 0.007099\n",
      "0.3579 --- loss: 0.007082\n",
      "0.3586 --- loss: 0.008402\n",
      "0.3593 --- loss: 0.010648\n",
      "0.3600 --- loss: 0.013448\n",
      "0.3607 --- loss: 0.008706\n",
      "0.3614 --- loss: 0.006483\n",
      "0.3621 --- loss: 0.007675\n",
      "0.3629 --- loss: 0.006079\n",
      "0.3636 --- loss: 0.006890\n",
      "0.3643 --- loss: 0.007731\n",
      "0.3650 --- loss: 0.019153\n",
      "0.3657 --- loss: 0.006874\n",
      "0.3664 --- loss: 0.010659\n",
      "0.3671 --- loss: 0.011206\n",
      "0.3679 --- loss: 0.007867\n",
      "0.3686 --- loss: 0.008211\n",
      "0.3693 --- loss: 0.007348\n",
      "0.3700 --- loss: 0.010712\n",
      "0.3707 --- loss: 0.007781\n",
      "0.3714 --- loss: 0.011346\n",
      "0.3721 --- loss: 0.008760\n",
      "0.3729 --- loss: 0.015007\n",
      "0.3736 --- loss: 0.011117\n",
      "0.3743 --- loss: 0.013502\n",
      "0.3750 --- loss: 0.006548\n",
      "0.3757 --- loss: 0.010382\n",
      "0.3764 --- loss: 0.008937\n",
      "0.3771 --- loss: 0.005513\n",
      "0.3779 --- loss: 0.014435\n",
      "0.3786 --- loss: 0.008917\n",
      "0.3793 --- loss: 0.008384\n",
      "0.3800 --- loss: 0.007460\n",
      "0.3807 --- loss: 0.011238\n",
      "0.3814 --- loss: 0.006841\n",
      "0.3821 --- loss: 0.010267\n",
      "0.3829 --- loss: 0.012889\n",
      "0.3836 --- loss: 0.010698\n",
      "0.3843 --- loss: 0.009491\n",
      "0.3850 --- loss: 0.006496\n",
      "0.3857 --- loss: 0.008628\n",
      "0.3864 --- loss: 0.009321\n",
      "0.3871 --- loss: 0.008921\n",
      "0.3879 --- loss: 0.007585\n",
      "0.3886 --- loss: 0.010953\n",
      "0.3893 --- loss: 0.013789\n",
      "0.3900 --- loss: 0.010118\n",
      "0.3907 --- loss: 0.007458\n",
      "0.3914 --- loss: 0.007802\n",
      "0.3921 --- loss: 0.008326\n",
      "0.3929 --- loss: 0.013823\n",
      "0.3936 --- loss: 0.007438\n",
      "0.3943 --- loss: 0.008631\n",
      "0.3950 --- loss: 0.007993\n",
      "0.3957 --- loss: 0.008230\n",
      "0.3964 --- loss: 0.011381\n",
      "0.3971 --- loss: 0.009828\n",
      "0.3979 --- loss: 0.011928\n",
      "0.3986 --- loss: 0.008357\n",
      "0.3993 --- loss: 0.007701\n",
      "0.4000 --- loss: 0.011723\n",
      "0.4007 --- loss: 0.009277\n",
      "0.4014 --- loss: 0.008086\n",
      "0.4021 --- loss: 0.010141\n",
      "0.4029 --- loss: 0.007081\n",
      "0.4036 --- loss: 0.008496\n",
      "0.4043 --- loss: 0.012201\n",
      "0.4050 --- loss: 0.018773\n",
      "0.4057 --- loss: 0.018620\n",
      "0.4064 --- loss: 0.008226\n",
      "0.4071 --- loss: 0.017671\n",
      "0.4079 --- loss: 0.009157\n",
      "0.4086 --- loss: 0.008534\n",
      "0.4093 --- loss: 0.007103\n",
      "0.4100 --- loss: 0.013811\n",
      "0.4107 --- loss: 0.017389\n",
      "0.4114 --- loss: 0.008033\n",
      "0.4121 --- loss: 0.013628\n",
      "0.4129 --- loss: 0.008934\n",
      "0.4136 --- loss: 0.010197\n",
      "0.4143 --- loss: 0.007175\n",
      "0.4150 --- loss: 0.009709\n",
      "0.4157 --- loss: 0.008466\n",
      "0.4164 --- loss: 0.008084\n",
      "0.4171 --- loss: 0.012867\n",
      "0.4179 --- loss: 0.007282\n",
      "0.4186 --- loss: 0.010033\n",
      "0.4193 --- loss: 0.005565\n",
      "0.4200 --- loss: 0.011103\n",
      "0.4207 --- loss: 0.011164\n",
      "0.4214 --- loss: 0.013428\n",
      "0.4221 --- loss: 0.008867\n",
      "0.4229 --- loss: 0.010357\n",
      "0.4236 --- loss: 0.008615\n",
      "0.4243 --- loss: 0.013374\n",
      "0.4250 --- loss: 0.009815\n",
      "0.4257 --- loss: 0.007978\n",
      "0.4264 --- loss: 0.007194\n",
      "0.4271 --- loss: 0.009198\n",
      "0.4279 --- loss: 0.009869\n",
      "0.4286 --- loss: 0.007606\n",
      "0.4293 --- loss: 0.012737\n",
      "0.4300 --- loss: 0.010220\n",
      "0.4307 --- loss: 0.007001\n",
      "0.4314 --- loss: 0.014313\n",
      "0.4321 --- loss: 0.010458\n",
      "0.4329 --- loss: 0.008383\n",
      "0.4336 --- loss: 0.008163\n",
      "0.4343 --- loss: 0.009699\n",
      "0.4350 --- loss: 0.013854\n",
      "0.4357 --- loss: 0.006379\n",
      "0.4364 --- loss: 0.007891\n",
      "0.4371 --- loss: 0.012698\n",
      "0.4379 --- loss: 0.009042\n",
      "0.4386 --- loss: 0.007939\n",
      "0.4393 --- loss: 0.019180\n",
      "0.4400 --- loss: 0.006034\n",
      "0.4407 --- loss: 0.007333\n",
      "0.4414 --- loss: 0.008165\n",
      "0.4421 --- loss: 0.006547\n",
      "0.4429 --- loss: 0.006977\n",
      "0.4436 --- loss: 0.009440\n",
      "0.4443 --- loss: 0.006838\n",
      "0.4450 --- loss: 0.006949\n",
      "0.4457 --- loss: 0.013064\n",
      "0.4464 --- loss: 0.011059\n",
      "0.4471 --- loss: 0.007603\n",
      "0.4479 --- loss: 0.007011\n",
      "0.4486 --- loss: 0.007485\n",
      "0.4493 --- loss: 0.007894\n",
      "0.4500 --- loss: 0.009431\n",
      "0.4507 --- loss: 0.009312\n",
      "0.4514 --- loss: 0.007418\n",
      "0.4521 --- loss: 0.014041\n",
      "0.4529 --- loss: 0.008055\n",
      "0.4536 --- loss: 0.008833\n",
      "0.4543 --- loss: 0.009112\n",
      "0.4550 --- loss: 0.008138\n",
      "0.4557 --- loss: 0.007838\n",
      "0.4564 --- loss: 0.010489\n",
      "0.4571 --- loss: 0.012683\n",
      "0.4579 --- loss: 0.009572\n",
      "0.4586 --- loss: 0.009221\n",
      "0.4593 --- loss: 0.007707\n",
      "0.4600 --- loss: 0.012058\n",
      "0.4607 --- loss: 0.009411\n",
      "0.4614 --- loss: 0.006011\n",
      "0.4621 --- loss: 0.009010\n",
      "0.4629 --- loss: 0.009967\n",
      "0.4636 --- loss: 0.007502\n",
      "0.4643 --- loss: 0.007953\n",
      "0.4650 --- loss: 0.013344\n",
      "0.4657 --- loss: 0.008142\n",
      "0.4664 --- loss: 0.005404\n",
      "0.4671 --- loss: 0.009028\n",
      "0.4679 --- loss: 0.008083\n",
      "0.4686 --- loss: 0.008402\n",
      "0.4693 --- loss: 0.006517\n",
      "0.4700 --- loss: 0.011591\n",
      "0.4707 --- loss: 0.007857\n",
      "0.4714 --- loss: 0.008686\n",
      "0.4721 --- loss: 0.010601\n",
      "0.4729 --- loss: 0.010180\n",
      "0.4736 --- loss: 0.008730\n",
      "0.4743 --- loss: 0.016535\n",
      "0.4750 --- loss: 0.010506\n",
      "0.4757 --- loss: 0.009909\n",
      "0.4764 --- loss: 0.011455\n",
      "0.4771 --- loss: 0.007320\n",
      "0.4779 --- loss: 0.007134\n",
      "0.4786 --- loss: 0.007595\n",
      "0.4793 --- loss: 0.010576\n",
      "0.4800 --- loss: 0.009277\n",
      "0.4807 --- loss: 0.008670\n",
      "0.4814 --- loss: 0.010236\n",
      "0.4821 --- loss: 0.006830\n",
      "0.4829 --- loss: 0.010737\n",
      "0.4836 --- loss: 0.011841\n",
      "0.4843 --- loss: 0.007761\n",
      "0.4850 --- loss: 0.010486\n",
      "0.4857 --- loss: 0.006884\n",
      "0.4864 --- loss: 0.013391\n",
      "0.4871 --- loss: 0.004852\n",
      "0.4879 --- loss: 0.008843\n",
      "0.4886 --- loss: 0.018149\n",
      "0.4893 --- loss: 0.006283\n",
      "0.4900 --- loss: 0.009397\n",
      "0.4907 --- loss: 0.011082\n",
      "0.4914 --- loss: 0.009818\n",
      "0.4921 --- loss: 0.009734\n",
      "0.4929 --- loss: 0.005668\n",
      "0.4936 --- loss: 0.010686\n",
      "0.4943 --- loss: 0.007987\n",
      "0.4950 --- loss: 0.008759\n",
      "0.4957 --- loss: 0.011405\n",
      "0.4964 --- loss: 0.008644\n",
      "0.4971 --- loss: 0.006593\n",
      "0.4979 --- loss: 0.009142\n",
      "0.4986 --- loss: 0.007394\n",
      "0.4993 --- loss: 0.011536\n",
      "0.5000 --- loss: 0.012273\n",
      "0.5007 --- loss: 0.010082\n",
      "0.5014 --- loss: 0.012718\n",
      "0.5021 --- loss: 0.008392\n",
      "0.5029 --- loss: 0.008557\n",
      "0.5036 --- loss: 0.010915\n",
      "0.5043 --- loss: 0.013722\n",
      "0.5050 --- loss: 0.011440\n",
      "0.5057 --- loss: 0.009975\n",
      "0.5064 --- loss: 0.009215\n",
      "0.5071 --- loss: 0.006192\n",
      "0.5079 --- loss: 0.010920\n",
      "0.5086 --- loss: 0.008882\n",
      "0.5093 --- loss: 0.008776\n",
      "0.5100 --- loss: 0.009287\n",
      "0.5107 --- loss: 0.008771\n",
      "0.5114 --- loss: 0.011034\n",
      "0.5121 --- loss: 0.008899\n",
      "0.5129 --- loss: 0.006298\n",
      "0.5136 --- loss: 0.008377\n",
      "0.5143 --- loss: 0.010640\n",
      "0.5150 --- loss: 0.007388\n",
      "0.5157 --- loss: 0.012471\n",
      "0.5164 --- loss: 0.004987\n",
      "0.5171 --- loss: 0.007568\n",
      "0.5179 --- loss: 0.009817\n",
      "0.5186 --- loss: 0.007533\n",
      "0.5193 --- loss: 0.005039\n",
      "0.5200 --- loss: 0.008464\n",
      "0.5207 --- loss: 0.007278\n",
      "0.5214 --- loss: 0.009821\n",
      "0.5221 --- loss: 0.005931\n",
      "0.5229 --- loss: 0.009013\n",
      "0.5236 --- loss: 0.009411\n",
      "0.5243 --- loss: 0.009021\n",
      "0.5250 --- loss: 0.015447\n",
      "0.5257 --- loss: 0.008201\n",
      "0.5264 --- loss: 0.008267\n",
      "0.5271 --- loss: 0.009979\n",
      "0.5279 --- loss: 0.012049\n",
      "0.5286 --- loss: 0.009756\n",
      "0.5293 --- loss: 0.006584\n",
      "0.5300 --- loss: 0.008011\n",
      "0.5307 --- loss: 0.008276\n",
      "0.5314 --- loss: 0.007782\n",
      "0.5321 --- loss: 0.008512\n",
      "0.5329 --- loss: 0.012218\n",
      "0.5336 --- loss: 0.007135\n",
      "0.5343 --- loss: 0.013314\n",
      "0.5350 --- loss: 0.008157\n",
      "0.5357 --- loss: 0.009403\n",
      "0.5364 --- loss: 0.017799\n",
      "0.5371 --- loss: 0.007929\n",
      "0.5379 --- loss: 0.008073\n",
      "0.5386 --- loss: 0.011232\n",
      "0.5393 --- loss: 0.008227\n",
      "0.5400 --- loss: 0.006407\n",
      "0.5407 --- loss: 0.009441\n",
      "0.5414 --- loss: 0.007365\n",
      "0.5421 --- loss: 0.008840\n",
      "0.5429 --- loss: 0.009178\n",
      "0.5436 --- loss: 0.005513\n",
      "0.5443 --- loss: 0.007287\n",
      "0.5450 --- loss: 0.007862\n",
      "0.5457 --- loss: 0.006769\n",
      "0.5464 --- loss: 0.009774\n",
      "0.5471 --- loss: 0.008684\n",
      "0.5479 --- loss: 0.009543\n",
      "0.5486 --- loss: 0.011488\n",
      "0.5493 --- loss: 0.007263\n",
      "0.5500 --- loss: 0.010355\n",
      "0.5507 --- loss: 0.012045\n",
      "0.5514 --- loss: 0.013648\n",
      "0.5521 --- loss: 0.010540\n",
      "0.5529 --- loss: 0.008260\n",
      "0.5536 --- loss: 0.007985\n",
      "0.5543 --- loss: 0.010410\n",
      "0.5550 --- loss: 0.009930\n",
      "0.5557 --- loss: 0.008657\n",
      "0.5564 --- loss: 0.012160\n",
      "0.5571 --- loss: 0.008649\n",
      "0.5579 --- loss: 0.012424\n",
      "0.5586 --- loss: 0.007980\n",
      "0.5593 --- loss: 0.006097\n",
      "0.5600 --- loss: 0.007516\n",
      "0.5607 --- loss: 0.008980\n",
      "0.5614 --- loss: 0.005739\n",
      "0.5621 --- loss: 0.008444\n",
      "0.5629 --- loss: 0.007499\n",
      "0.5636 --- loss: 0.009471\n",
      "0.5643 --- loss: 0.013301\n",
      "0.5650 --- loss: 0.011745\n",
      "0.5657 --- loss: 0.012978\n",
      "0.5664 --- loss: 0.009736\n",
      "0.5671 --- loss: 0.009983\n",
      "0.5679 --- loss: 0.011409\n",
      "0.5686 --- loss: 0.009620\n",
      "0.5693 --- loss: 0.011461\n",
      "0.5700 --- loss: 0.010162\n",
      "0.5707 --- loss: 0.009152\n",
      "0.5714 --- loss: 0.008662\n",
      "0.5721 --- loss: 0.007498\n",
      "0.5729 --- loss: 0.007532\n",
      "0.5736 --- loss: 0.010610\n",
      "0.5743 --- loss: 0.014840\n",
      "0.5750 --- loss: 0.007899\n",
      "0.5757 --- loss: 0.007403\n",
      "0.5764 --- loss: 0.008886\n",
      "0.5771 --- loss: 0.005620\n",
      "0.5779 --- loss: 0.007537\n",
      "0.5786 --- loss: 0.007055\n",
      "0.5793 --- loss: 0.009531\n",
      "0.5800 --- loss: 0.006777\n",
      "0.5807 --- loss: 0.009179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5814 --- loss: 0.006129\n",
      "0.5821 --- loss: 0.007352\n",
      "0.5829 --- loss: 0.005163\n",
      "0.5836 --- loss: 0.013496\n",
      "0.5843 --- loss: 0.007748\n",
      "0.5850 --- loss: 0.010129\n",
      "0.5857 --- loss: 0.008548\n",
      "0.5864 --- loss: 0.006754\n",
      "0.5871 --- loss: 0.007591\n",
      "0.5879 --- loss: 0.008628\n",
      "0.5886 --- loss: 0.006746\n",
      "0.5893 --- loss: 0.006970\n",
      "0.5900 --- loss: 0.009457\n",
      "0.5907 --- loss: 0.009351\n",
      "0.5914 --- loss: 0.007157\n",
      "0.5921 --- loss: 0.014421\n",
      "0.5929 --- loss: 0.008018\n",
      "0.5936 --- loss: 0.007652\n",
      "0.5943 --- loss: 0.010723\n",
      "0.5950 --- loss: 0.016404\n",
      "0.5957 --- loss: 0.008635\n",
      "0.5964 --- loss: 0.008539\n",
      "0.5971 --- loss: 0.011182\n",
      "0.5979 --- loss: 0.009720\n",
      "0.5986 --- loss: 0.005163\n",
      "0.5993 --- loss: 0.011500\n",
      "0.6000 --- loss: 0.009061\n",
      "0.6007 --- loss: 0.011559\n",
      "0.6014 --- loss: 0.013437\n",
      "0.6021 --- loss: 0.006991\n",
      "0.6029 --- loss: 0.007870\n",
      "0.6036 --- loss: 0.009437\n",
      "0.6043 --- loss: 0.008930\n",
      "0.6050 --- loss: 0.007231\n",
      "0.6057 --- loss: 0.008104\n",
      "0.6064 --- loss: 0.010442\n",
      "0.6071 --- loss: 0.008513\n",
      "0.6079 --- loss: 0.012099\n",
      "0.6086 --- loss: 0.007624\n",
      "0.6093 --- loss: 0.009183\n",
      "0.6100 --- loss: 0.009407\n",
      "0.6107 --- loss: 0.006289\n",
      "0.6114 --- loss: 0.009880\n",
      "0.6121 --- loss: 0.011473\n",
      "0.6129 --- loss: 0.012695\n",
      "0.6136 --- loss: 0.006184\n",
      "0.6143 --- loss: 0.012155\n",
      "0.6150 --- loss: 0.013381\n",
      "0.6157 --- loss: 0.008773\n",
      "0.6164 --- loss: 0.010200\n",
      "0.6171 --- loss: 0.012078\n",
      "0.6179 --- loss: 0.008172\n",
      "0.6186 --- loss: 0.008566\n",
      "0.6193 --- loss: 0.012292\n",
      "0.6200 --- loss: 0.010276\n",
      "0.6207 --- loss: 0.006417\n",
      "0.6214 --- loss: 0.010893\n",
      "0.6221 --- loss: 0.012598\n",
      "0.6229 --- loss: 0.006767\n",
      "0.6236 --- loss: 0.008975\n",
      "0.6243 --- loss: 0.008532\n",
      "0.6250 --- loss: 0.011146\n",
      "0.6257 --- loss: 0.006068\n",
      "0.6264 --- loss: 0.009432\n",
      "0.6271 --- loss: 0.011033\n",
      "0.6279 --- loss: 0.011650\n",
      "0.6286 --- loss: 0.007677\n",
      "0.6293 --- loss: 0.009561\n",
      "0.6300 --- loss: 0.006934\n",
      "0.6307 --- loss: 0.011041\n",
      "0.6314 --- loss: 0.010255\n",
      "0.6321 --- loss: 0.008559\n",
      "0.6329 --- loss: 0.006033\n",
      "0.6336 --- loss: 0.016899\n",
      "0.6343 --- loss: 0.007157\n",
      "0.6350 --- loss: 0.008211\n",
      "0.6357 --- loss: 0.008177\n",
      "0.6364 --- loss: 0.009372\n",
      "0.6371 --- loss: 0.008766\n",
      "0.6379 --- loss: 0.016390\n",
      "0.6386 --- loss: 0.009285\n",
      "0.6393 --- loss: 0.010869\n",
      "0.6400 --- loss: 0.009124\n",
      "0.6407 --- loss: 0.009393\n",
      "0.6414 --- loss: 0.007667\n",
      "0.6421 --- loss: 0.008359\n",
      "0.6429 --- loss: 0.013103\n",
      "0.6436 --- loss: 0.006797\n",
      "0.6443 --- loss: 0.008554\n",
      "0.6450 --- loss: 0.007755\n",
      "0.6457 --- loss: 0.010440\n",
      "0.6464 --- loss: 0.010022\n",
      "0.6471 --- loss: 0.008479\n",
      "0.6479 --- loss: 0.008830\n",
      "0.6486 --- loss: 0.008383\n",
      "0.6493 --- loss: 0.007814\n",
      "0.6500 --- loss: 0.010115\n",
      "0.6507 --- loss: 0.008343\n",
      "0.6514 --- loss: 0.007387\n",
      "0.6521 --- loss: 0.009512\n",
      "0.6529 --- loss: 0.009219\n",
      "0.6536 --- loss: 0.009008\n",
      "0.6543 --- loss: 0.009324\n",
      "0.6550 --- loss: 0.014473\n",
      "0.6557 --- loss: 0.012068\n",
      "0.6564 --- loss: 0.009604\n",
      "0.6571 --- loss: 0.009010\n",
      "0.6579 --- loss: 0.007404\n",
      "0.6586 --- loss: 0.011422\n",
      "0.6593 --- loss: 0.007426\n",
      "0.6600 --- loss: 0.010570\n",
      "0.6607 --- loss: 0.008968\n",
      "0.6614 --- loss: 0.013295\n",
      "0.6621 --- loss: 0.009682\n",
      "0.6629 --- loss: 0.009325\n",
      "0.6636 --- loss: 0.010020\n",
      "0.6643 --- loss: 0.006855\n",
      "0.6650 --- loss: 0.007303\n",
      "0.6657 --- loss: 0.012514\n",
      "0.6664 --- loss: 0.006426\n",
      "0.6671 --- loss: 0.007886\n",
      "0.6679 --- loss: 0.009464\n",
      "0.6686 --- loss: 0.007805\n",
      "0.6693 --- loss: 0.010009\n",
      "0.6700 --- loss: 0.012618\n",
      "0.6707 --- loss: 0.005312\n",
      "0.6714 --- loss: 0.008301\n",
      "0.6721 --- loss: 0.008125\n",
      "0.6729 --- loss: 0.009090\n",
      "0.6736 --- loss: 0.006602\n",
      "0.6743 --- loss: 0.010561\n",
      "0.6750 --- loss: 0.009085\n",
      "0.6757 --- loss: 0.007929\n",
      "0.6764 --- loss: 0.011671\n",
      "0.6771 --- loss: 0.009234\n",
      "0.6779 --- loss: 0.009682\n",
      "0.6786 --- loss: 0.007011\n",
      "0.6793 --- loss: 0.008378\n",
      "0.6800 --- loss: 0.006783\n",
      "0.6807 --- loss: 0.006867\n",
      "0.6814 --- loss: 0.007801\n",
      "0.6821 --- loss: 0.012758\n",
      "0.6829 --- loss: 0.010345\n",
      "0.6836 --- loss: 0.010417\n",
      "0.6843 --- loss: 0.006782\n",
      "0.6850 --- loss: 0.007972\n",
      "0.6857 --- loss: 0.007090\n",
      "0.6864 --- loss: 0.010834\n",
      "0.6871 --- loss: 0.005632\n",
      "0.6879 --- loss: 0.012265\n",
      "0.6886 --- loss: 0.009694\n",
      "0.6893 --- loss: 0.008599\n",
      "0.6900 --- loss: 0.008243\n",
      "0.6907 --- loss: 0.011282\n",
      "0.6914 --- loss: 0.008139\n",
      "0.6921 --- loss: 0.013508\n",
      "0.6929 --- loss: 0.007008\n",
      "0.6936 --- loss: 0.011753\n",
      "0.6943 --- loss: 0.011409\n",
      "0.6950 --- loss: 0.010131\n",
      "0.6957 --- loss: 0.007473\n",
      "0.6964 --- loss: 0.009786\n",
      "0.6971 --- loss: 0.006426\n",
      "0.6979 --- loss: 0.010161\n",
      "0.6986 --- loss: 0.008011\n",
      "0.6993 --- loss: 0.007255\n",
      "0.7000 --- loss: 0.010591\n",
      "0.7007 --- loss: 0.010922\n",
      "0.7014 --- loss: 0.010034\n",
      "0.7021 --- loss: 0.006246\n",
      "0.7029 --- loss: 0.010932\n",
      "0.7036 --- loss: 0.009050\n",
      "0.7043 --- loss: 0.008159\n",
      "0.7050 --- loss: 0.008503\n",
      "0.7057 --- loss: 0.008941\n",
      "0.7064 --- loss: 0.008651\n",
      "0.7071 --- loss: 0.011415\n",
      "0.7079 --- loss: 0.005902\n",
      "0.7086 --- loss: 0.013275\n",
      "0.7093 --- loss: 0.010253\n",
      "0.7100 --- loss: 0.010623\n",
      "0.7107 --- loss: 0.008274\n",
      "0.7114 --- loss: 0.009185\n",
      "0.7121 --- loss: 0.007864\n",
      "0.7129 --- loss: 0.006191\n",
      "0.7136 --- loss: 0.008065\n",
      "0.7143 --- loss: 0.012354\n",
      "0.7150 --- loss: 0.009725\n",
      "0.7157 --- loss: 0.007875\n",
      "0.7164 --- loss: 0.008368\n",
      "0.7171 --- loss: 0.011216\n",
      "0.7179 --- loss: 0.011095\n",
      "0.7186 --- loss: 0.008687\n",
      "0.7193 --- loss: 0.007706\n",
      "0.7200 --- loss: 0.009330\n",
      "0.7207 --- loss: 0.009048\n",
      "0.7214 --- loss: 0.005970\n",
      "0.7221 --- loss: 0.005827\n",
      "0.7229 --- loss: 0.012235\n",
      "0.7236 --- loss: 0.006094\n",
      "0.7243 --- loss: 0.007128\n",
      "0.7250 --- loss: 0.007300\n",
      "0.7257 --- loss: 0.012298\n",
      "0.7264 --- loss: 0.004470\n",
      "0.7271 --- loss: 0.009364\n",
      "0.7279 --- loss: 0.012236\n",
      "0.7286 --- loss: 0.008384\n",
      "0.7293 --- loss: 0.005730\n",
      "0.7300 --- loss: 0.010576\n",
      "0.7307 --- loss: 0.006833\n",
      "0.7314 --- loss: 0.011965\n",
      "0.7321 --- loss: 0.008708\n",
      "0.7329 --- loss: 0.011535\n",
      "0.7336 --- loss: 0.005571\n",
      "0.7343 --- loss: 0.011138\n",
      "0.7350 --- loss: 0.009727\n",
      "0.7357 --- loss: 0.011046\n",
      "0.7364 --- loss: 0.009421\n",
      "0.7371 --- loss: 0.016270\n",
      "0.7379 --- loss: 0.006530\n",
      "0.7386 --- loss: 0.012088\n",
      "0.7393 --- loss: 0.006932\n",
      "0.7400 --- loss: 0.011439\n",
      "0.7407 --- loss: 0.009110\n",
      "0.7414 --- loss: 0.009259\n",
      "0.7421 --- loss: 0.006085\n",
      "0.7429 --- loss: 0.005538\n",
      "0.7436 --- loss: 0.006744\n",
      "0.7443 --- loss: 0.007818\n",
      "0.7450 --- loss: 0.007956\n",
      "0.7457 --- loss: 0.007277\n",
      "0.7464 --- loss: 0.006966\n",
      "0.7471 --- loss: 0.007188\n",
      "0.7479 --- loss: 0.010613\n",
      "0.7486 --- loss: 0.012460\n",
      "0.7493 --- loss: 0.009509\n",
      "0.7500 --- loss: 0.006311\n",
      "0.7507 --- loss: 0.008199\n",
      "0.7514 --- loss: 0.010516\n",
      "0.7521 --- loss: 0.005801\n",
      "0.7529 --- loss: 0.006928\n",
      "0.7536 --- loss: 0.006725\n",
      "0.7543 --- loss: 0.009919\n",
      "0.7550 --- loss: 0.010556\n",
      "0.7557 --- loss: 0.007985\n",
      "0.7564 --- loss: 0.005300\n",
      "0.7571 --- loss: 0.010352\n",
      "0.7579 --- loss: 0.010797\n",
      "0.7586 --- loss: 0.006664\n",
      "0.7593 --- loss: 0.006035\n",
      "0.7600 --- loss: 0.009460\n",
      "0.7607 --- loss: 0.010952\n",
      "0.7614 --- loss: 0.006937\n",
      "0.7621 --- loss: 0.007134\n",
      "0.7629 --- loss: 0.009235\n",
      "0.7636 --- loss: 0.010168\n",
      "0.7643 --- loss: 0.011666\n",
      "0.7650 --- loss: 0.007501\n",
      "0.7657 --- loss: 0.008411\n",
      "0.7664 --- loss: 0.008472\n",
      "0.7671 --- loss: 0.007297\n",
      "0.7679 --- loss: 0.017703\n",
      "0.7686 --- loss: 0.005798\n",
      "0.7693 --- loss: 0.006911\n",
      "0.7700 --- loss: 0.007436\n",
      "0.7707 --- loss: 0.005815\n",
      "0.7714 --- loss: 0.012950\n",
      "0.7721 --- loss: 0.008204\n",
      "0.7729 --- loss: 0.007060\n",
      "0.7736 --- loss: 0.007620\n",
      "0.7743 --- loss: 0.007822\n",
      "0.7750 --- loss: 0.006606\n",
      "0.7757 --- loss: 0.008839\n",
      "0.7764 --- loss: 0.009030\n",
      "0.7771 --- loss: 0.011207\n",
      "0.7779 --- loss: 0.010093\n",
      "0.7786 --- loss: 0.005970\n",
      "0.7793 --- loss: 0.007675\n",
      "0.7800 --- loss: 0.009177\n",
      "0.7807 --- loss: 0.007773\n",
      "0.7814 --- loss: 0.010568\n",
      "0.7821 --- loss: 0.008769\n",
      "0.7829 --- loss: 0.008280\n",
      "0.7836 --- loss: 0.009098\n",
      "0.7843 --- loss: 0.006197\n",
      "0.7850 --- loss: 0.009708\n",
      "0.7857 --- loss: 0.007269\n",
      "0.7864 --- loss: 0.009060\n",
      "0.7871 --- loss: 0.008599\n",
      "0.7879 --- loss: 0.007138\n",
      "0.7886 --- loss: 0.010576\n",
      "0.7893 --- loss: 0.010166\n",
      "0.7900 --- loss: 0.009780\n",
      "0.7907 --- loss: 0.009680\n",
      "0.7914 --- loss: 0.013025\n",
      "0.7921 --- loss: 0.007207\n",
      "0.7929 --- loss: 0.009212\n",
      "0.7936 --- loss: 0.008404\n",
      "0.7943 --- loss: 0.013180\n",
      "0.7950 --- loss: 0.007976\n",
      "0.7957 --- loss: 0.008491\n",
      "0.7964 --- loss: 0.006237\n",
      "0.7971 --- loss: 0.008069\n",
      "0.7979 --- loss: 0.009950\n",
      "0.7986 --- loss: 0.008537\n",
      "0.7993 --- loss: 0.007445\n",
      "0.8000 --- loss: 0.010714\n",
      "0.8007 --- loss: 0.007508\n",
      "0.8014 --- loss: 0.005538\n",
      "0.8021 --- loss: 0.008439\n",
      "0.8029 --- loss: 0.009333\n",
      "0.8036 --- loss: 0.008098\n",
      "0.8043 --- loss: 0.008427\n",
      "0.8050 --- loss: 0.012758\n",
      "0.8057 --- loss: 0.009916\n",
      "0.8064 --- loss: 0.010687\n",
      "0.8071 --- loss: 0.011505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8079 --- loss: 0.013310\n",
      "0.8086 --- loss: 0.007406\n",
      "0.8093 --- loss: 0.009556\n",
      "0.8100 --- loss: 0.007951\n",
      "0.8107 --- loss: 0.008580\n",
      "0.8114 --- loss: 0.016957\n",
      "0.8121 --- loss: 0.008692\n",
      "0.8129 --- loss: 0.007711\n",
      "0.8136 --- loss: 0.011506\n",
      "0.8143 --- loss: 0.009294\n",
      "0.8150 --- loss: 0.008721\n",
      "0.8157 --- loss: 0.011772\n",
      "0.8164 --- loss: 0.005038\n",
      "0.8171 --- loss: 0.005440\n",
      "0.8179 --- loss: 0.006507\n",
      "0.8186 --- loss: 0.009026\n",
      "0.8193 --- loss: 0.016347\n",
      "0.8200 --- loss: 0.009773\n",
      "0.8207 --- loss: 0.006352\n",
      "0.8214 --- loss: 0.007576\n",
      "0.8221 --- loss: 0.009321\n",
      "0.8229 --- loss: 0.011018\n",
      "0.8236 --- loss: 0.007814\n",
      "0.8243 --- loss: 0.006126\n",
      "0.8250 --- loss: 0.012129\n",
      "0.8257 --- loss: 0.008357\n",
      "0.8264 --- loss: 0.012108\n",
      "0.8271 --- loss: 0.010325\n",
      "0.8279 --- loss: 0.007572\n",
      "0.8286 --- loss: 0.007987\n",
      "0.8293 --- loss: 0.009277\n",
      "0.8300 --- loss: 0.016012\n",
      "0.8307 --- loss: 0.006297\n",
      "0.8314 --- loss: 0.006004\n",
      "0.8321 --- loss: 0.007391\n",
      "0.8329 --- loss: 0.007587\n",
      "0.8336 --- loss: 0.009501\n",
      "0.8343 --- loss: 0.010988\n",
      "0.8350 --- loss: 0.011406\n",
      "0.8357 --- loss: 0.009345\n",
      "0.8364 --- loss: 0.010649\n",
      "0.8371 --- loss: 0.008773\n",
      "0.8379 --- loss: 0.007875\n",
      "0.8386 --- loss: 0.005040\n",
      "0.8393 --- loss: 0.006395\n",
      "0.8400 --- loss: 0.008677\n",
      "0.8407 --- loss: 0.007207\n",
      "0.8414 --- loss: 0.008958\n",
      "0.8421 --- loss: 0.012019\n",
      "0.8429 --- loss: 0.008533\n",
      "0.8436 --- loss: 0.010098\n",
      "0.8443 --- loss: 0.008815\n",
      "0.8450 --- loss: 0.009467\n",
      "0.8457 --- loss: 0.010268\n",
      "0.8464 --- loss: 0.011340\n",
      "0.8471 --- loss: 0.009177\n",
      "0.8479 --- loss: 0.010428\n",
      "0.8486 --- loss: 0.008516\n",
      "0.8493 --- loss: 0.005841\n",
      "0.8500 --- loss: 0.008951\n",
      "0.8507 --- loss: 0.007732\n",
      "0.8514 --- loss: 0.010260\n",
      "0.8521 --- loss: 0.011243\n",
      "0.8529 --- loss: 0.010452\n",
      "0.8536 --- loss: 0.012913\n",
      "0.8543 --- loss: 0.009147\n",
      "0.8550 --- loss: 0.009773\n",
      "0.8557 --- loss: 0.010155\n",
      "0.8564 --- loss: 0.013226\n",
      "0.8571 --- loss: 0.011861\n",
      "0.8579 --- loss: 0.007688\n",
      "0.8586 --- loss: 0.005864\n",
      "0.8593 --- loss: 0.008659\n",
      "0.8600 --- loss: 0.011230\n",
      "0.8607 --- loss: 0.007395\n",
      "0.8614 --- loss: 0.005766\n",
      "0.8621 --- loss: 0.008376\n",
      "0.8629 --- loss: 0.005994\n",
      "0.8636 --- loss: 0.005269\n",
      "0.8643 --- loss: 0.005948\n",
      "0.8650 --- loss: 0.007864\n",
      "0.8657 --- loss: 0.008182\n",
      "0.8664 --- loss: 0.007998\n",
      "0.8671 --- loss: 0.008850\n",
      "0.8679 --- loss: 0.010113\n",
      "0.8686 --- loss: 0.010548\n",
      "0.8693 --- loss: 0.010791\n",
      "0.8700 --- loss: 0.011281\n",
      "0.8707 --- loss: 0.008189\n",
      "0.8714 --- loss: 0.011715\n",
      "0.8721 --- loss: 0.006444\n",
      "0.8729 --- loss: 0.007022\n",
      "0.8736 --- loss: 0.008422\n",
      "0.8743 --- loss: 0.009665\n",
      "0.8750 --- loss: 0.010213\n",
      "0.8757 --- loss: 0.004862\n",
      "0.8764 --- loss: 0.008818\n",
      "0.8771 --- loss: 0.008080\n",
      "0.8779 --- loss: 0.009665\n",
      "0.8786 --- loss: 0.011535\n",
      "0.8793 --- loss: 0.009583\n",
      "0.8800 --- loss: 0.011557\n",
      "0.8807 --- loss: 0.014176\n",
      "0.8814 --- loss: 0.010343\n",
      "0.8821 --- loss: 0.008793\n",
      "0.8829 --- loss: 0.006498\n",
      "0.8836 --- loss: 0.007884\n",
      "0.8843 --- loss: 0.008529\n",
      "0.8850 --- loss: 0.007202\n",
      "0.8857 --- loss: 0.008238\n",
      "0.8864 --- loss: 0.009822\n",
      "0.8871 --- loss: 0.007694\n",
      "0.8879 --- loss: 0.009507\n",
      "0.8886 --- loss: 0.006148\n",
      "0.8893 --- loss: 0.007062\n",
      "0.8900 --- loss: 0.006890\n",
      "0.8907 --- loss: 0.012486\n",
      "0.8914 --- loss: 0.009379\n",
      "0.8921 --- loss: 0.007320\n",
      "0.8929 --- loss: 0.007224\n",
      "0.8936 --- loss: 0.007017\n",
      "0.8943 --- loss: 0.008106\n",
      "0.8950 --- loss: 0.007570\n",
      "0.8957 --- loss: 0.012372\n",
      "0.8964 --- loss: 0.013290\n",
      "0.8971 --- loss: 0.007919\n",
      "0.8979 --- loss: 0.011380\n",
      "0.8986 --- loss: 0.011933\n",
      "0.8993 --- loss: 0.013485\n",
      "0.9000 --- loss: 0.006862\n",
      "0.9007 --- loss: 0.011864\n",
      "0.9014 --- loss: 0.010293\n",
      "0.9021 --- loss: 0.008205\n",
      "0.9029 --- loss: 0.006411\n",
      "0.9036 --- loss: 0.011398\n",
      "0.9043 --- loss: 0.010397\n",
      "0.9050 --- loss: 0.010297\n",
      "0.9057 --- loss: 0.009614\n",
      "0.9064 --- loss: 0.008304\n",
      "0.9071 --- loss: 0.006692\n",
      "0.9079 --- loss: 0.012578\n",
      "0.9086 --- loss: 0.011449\n",
      "0.9093 --- loss: 0.014191\n",
      "0.9100 --- loss: 0.010704\n",
      "0.9107 --- loss: 0.005515\n",
      "0.9114 --- loss: 0.009644\n",
      "0.9121 --- loss: 0.009836\n",
      "0.9129 --- loss: 0.013028\n",
      "0.9136 --- loss: 0.009834\n",
      "0.9143 --- loss: 0.005423\n",
      "0.9150 --- loss: 0.008399\n",
      "0.9157 --- loss: 0.009749\n",
      "0.9164 --- loss: 0.012158\n",
      "0.9171 --- loss: 0.014214\n",
      "0.9179 --- loss: 0.011228\n",
      "0.9186 --- loss: 0.006240\n",
      "0.9193 --- loss: 0.011783\n",
      "0.9200 --- loss: 0.007477\n",
      "0.9207 --- loss: 0.011693\n",
      "0.9214 --- loss: 0.007123\n",
      "0.9221 --- loss: 0.010312\n",
      "0.9229 --- loss: 0.008140\n",
      "0.9236 --- loss: 0.014993\n",
      "0.9243 --- loss: 0.009239\n",
      "0.9250 --- loss: 0.007218\n",
      "0.9257 --- loss: 0.005031\n",
      "0.9264 --- loss: 0.009502\n",
      "0.9271 --- loss: 0.007961\n",
      "0.9279 --- loss: 0.007502\n",
      "0.9286 --- loss: 0.006371\n",
      "0.9293 --- loss: 0.008122\n",
      "0.9300 --- loss: 0.008796\n",
      "0.9307 --- loss: 0.006911\n",
      "0.9314 --- loss: 0.008420\n",
      "0.9321 --- loss: 0.006211\n",
      "0.9329 --- loss: 0.012084\n",
      "0.9336 --- loss: 0.006250\n",
      "0.9343 --- loss: 0.008267\n",
      "0.9350 --- loss: 0.008685\n",
      "0.9357 --- loss: 0.008923\n",
      "0.9364 --- loss: 0.010123\n",
      "0.9371 --- loss: 0.010508\n",
      "0.9379 --- loss: 0.007277\n",
      "0.9386 --- loss: 0.009601\n",
      "0.9393 --- loss: 0.009779\n",
      "0.9400 --- loss: 0.012095\n",
      "0.9407 --- loss: 0.005736\n",
      "0.9414 --- loss: 0.013421\n",
      "0.9421 --- loss: 0.006696\n",
      "0.9429 --- loss: 0.009973\n",
      "0.9436 --- loss: 0.007696\n",
      "0.9443 --- loss: 0.008485\n",
      "0.9450 --- loss: 0.008461\n",
      "0.9457 --- loss: 0.008919\n",
      "0.9464 --- loss: 0.007911\n",
      "0.9471 --- loss: 0.009743\n",
      "0.9479 --- loss: 0.008484\n",
      "0.9486 --- loss: 0.007284\n",
      "0.9493 --- loss: 0.007535\n",
      "0.9500 --- loss: 0.009842\n",
      "0.9507 --- loss: 0.007912\n",
      "0.9514 --- loss: 0.011974\n",
      "0.9521 --- loss: 0.008592\n",
      "0.9529 --- loss: 0.010365\n",
      "0.9536 --- loss: 0.008285\n",
      "0.9543 --- loss: 0.008413\n",
      "0.9550 --- loss: 0.006691\n",
      "0.9557 --- loss: 0.007137\n",
      "0.9564 --- loss: 0.009347\n",
      "0.9571 --- loss: 0.009028\n",
      "0.9579 --- loss: 0.008630\n",
      "0.9586 --- loss: 0.008694\n",
      "0.9593 --- loss: 0.009768\n",
      "0.9600 --- loss: 0.010756\n",
      "0.9607 --- loss: 0.007663\n",
      "0.9614 --- loss: 0.009356\n",
      "0.9621 --- loss: 0.007097\n",
      "0.9629 --- loss: 0.009111\n",
      "0.9636 --- loss: 0.012235\n",
      "0.9643 --- loss: 0.005908\n",
      "0.9650 --- loss: 0.008870\n",
      "0.9657 --- loss: 0.012618\n",
      "0.9664 --- loss: 0.008223\n",
      "0.9671 --- loss: 0.009897\n",
      "0.9679 --- loss: 0.007935\n",
      "0.9686 --- loss: 0.008326\n",
      "0.9693 --- loss: 0.011871\n",
      "0.9700 --- loss: 0.006533\n",
      "0.9707 --- loss: 0.010851\n",
      "0.9714 --- loss: 0.008959\n",
      "0.9721 --- loss: 0.006624\n",
      "0.9729 --- loss: 0.011666\n",
      "0.9736 --- loss: 0.006466\n",
      "0.9743 --- loss: 0.009802\n",
      "0.9750 --- loss: 0.012189\n",
      "0.9757 --- loss: 0.006974\n",
      "0.9764 --- loss: 0.008557\n",
      "0.9771 --- loss: 0.007700\n",
      "0.9779 --- loss: 0.011322\n",
      "0.9786 --- loss: 0.006244\n",
      "0.9793 --- loss: 0.009694\n",
      "0.9800 --- loss: 0.011489\n",
      "0.9807 --- loss: 0.005699\n",
      "0.9814 --- loss: 0.005677\n",
      "0.9821 --- loss: 0.006513\n",
      "0.9829 --- loss: 0.009300\n",
      "0.9836 --- loss: 0.009940\n",
      "0.9843 --- loss: 0.012394\n",
      "0.9850 --- loss: 0.019007\n",
      "0.9857 --- loss: 0.008518\n",
      "0.9864 --- loss: 0.010237\n",
      "0.9871 --- loss: 0.007554\n",
      "0.9879 --- loss: 0.011486\n",
      "0.9886 --- loss: 0.009649\n",
      "0.9893 --- loss: 0.006511\n",
      "0.9900 --- loss: 0.011585\n",
      "0.9907 --- loss: 0.006347\n",
      "0.9914 --- loss: 0.007543\n",
      "0.9921 --- loss: 0.008309\n",
      "0.9929 --- loss: 0.008462\n",
      "0.9936 --- loss: 0.008768\n",
      "0.9943 --- loss: 0.007627\n",
      "0.9950 --- loss: 0.009843\n",
      "0.9957 --- loss: 0.008912\n",
      "0.9964 --- loss: 0.011143\n",
      "0.9971 --- loss: 0.008745\n",
      "0.9979 --- loss: 0.009578\n",
      "0.9986 --- loss: 0.008162\n",
      "0.9993 --- loss: 0.006678\n",
      "Epoch finished ! Loss: 0.009332258914669895\n",
      "Validation Dice Coeff: tensor([0.0100], grad_fn=<DivBackward0>)\n",
      "Checkpoint 2 saved !\n"
     ]
    }
   ],
   "source": [
    "################################################ [TODO] ###################################################\n",
    "# Specify number of epochs, image scale factor, batch size and learning rate\n",
    "epochs = 2\n",
    "\n",
    "# i.e, 2\n",
    "batch_size = 50 # i.e, 16\n",
    "lr = 0.01        # i.e, 0.01\n",
    "lr_lambda = lambda epoch: 0.95 ** epoch\n",
    "N_train = len(train_dataset)\n",
    "model_save_path = './model/'  # directory to same the model after each epoch. \n",
    "\n",
    "\n",
    "################################################ [TODO] ###################################################\n",
    "# Define an optimizer for your model.\n",
    "# Pytorch has built-in package called optim. Most commonly used methods are already supported.\n",
    "# Here we use stochastic gradient descent to optimize\n",
    "# For usage of SGD, you can read https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html\n",
    "# Also you can use ADAM as the optimizer\n",
    "# For usage of ADAM, you can read https://www.programcreek.com/python/example/92667/torch.optim.Adam\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "#suggested parameter settings: momentum=0.9, weight_decay=0.0005\n",
    "\n",
    "#OR optimizer = optim.Adam(...)\n",
    "\n",
    "\n",
    "# The loss function we use is binary cross entropy: nn.BCELoss()\n",
    "# criterion = nn.MSELoss()\n",
    "# note that although we want to use DICE for evaluation, we use BCELoss for training in this example\n",
    "\n",
    "################################################ [TODO] ###################################################\n",
    "# Start training\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch {}/{}.'.format(epoch + 1, epochs))\n",
    "    scheduler.step()\n",
    "    net.train()\n",
    "    # Reload images and masks for training and validation and perform random shuffling at the begining of each epoch\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    epoch_loss = 0\n",
    "    for i, b in enumerate(train_loader):\n",
    "        ################################################ [TODO] ###################################################\n",
    "        # Get images and masks from each batch\n",
    "        imgs = b['img']\n",
    "        label = b['label']\n",
    "        imgs = imgs.type(torch.FloatTensor);\n",
    "        ################################################ [TODO] ###################################################\n",
    "        # Feed your images into the network\n",
    "        img_pred = net.forward(imgs)\n",
    "        #print(img_pred.shape)\n",
    "        #print(label.shape)\n",
    "        # Flatten the predicted masks and true masks. For example, A_flat = A.view(-1)\n",
    "        img_pred_flat = img_pred.view(-1)\n",
    "        label_flat = label.view(-1)\n",
    "        ################################################ [TODO] ###################################################\n",
    "        # Calculate the loss by comparing the predicted masks vector and true masks vector\n",
    "        # And sum the losses together \n",
    "        loss = grad_loss(img_pred, label, epoch)\n",
    "        #loss = criterion(img_pred_flat, label_flat)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        print('{0:.4f} --- loss: {1:.6f}'.format(i * batch_size / N_train, loss.item()))\n",
    "\n",
    "        # optimizer.zero_grad() clears x.grad for every parameter x in the optimizer. \n",
    "        # Its important to call this before loss.backward(), otherwise youll accumulate the gradients from multiple passes.\n",
    "        optimizer.zero_grad()\n",
    "        # loss.backward() computes dloss/dx for every parameter x which has requires_grad=True. \n",
    "        # These are accumulated into x.grad for every parameter x\n",
    "        # x.grad += dloss/dx\n",
    "        loss.backward()\n",
    "        # optimizer.step updates the value of x using the gradient x.grad. \n",
    "        # x += -lr * x.grad\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Epoch finished ! Loss: {}'.format(epoch_loss / i))\n",
    "    ################################################ [TODO] ###################################################\n",
    "    # Perform validation with eval_net() on the validation data\n",
    "    val_dice = eval_net(net, val_loader)\n",
    "    print('Validation Dice Coeff: {}'.format(val_dice))\n",
    "    # Save the model after each epoch\n",
    "    if os.path.isdir(model_save_path):\n",
    "        torch.save(net.state_dict(),model_save_path + 'Car_Seg_Epoch{}.pth'.format(epoch + 1))\n",
    "    else:\n",
    "        os.makedirs(model_save_path, exist_ok=True)\n",
    "        torch.save(net.state_dict(),model_save_path + 'Car_Seg_Epoch{}.pth'.format(epoch + 1))\n",
    "    print('Checkpoint {} saved !'.format(epoch + 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ TODO 4 ] load one image from testing dataset and plot output mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################ [TODO] ###################################################\n",
    "# Define a function for prediction/testing\n",
    "def predict_img(net,full_img):\n",
    "    # set the mode of your network to evaluation\n",
    "    net.eval()\n",
    "    \n",
    "    # convert from Height*Width*Channel TO Channel*Height*Width\n",
    "    # convert numpy array to torch tensor \n",
    "    #Unsqueeze add an extra dimension\n",
    "    X_img = torch.from_numpy(full_img).unsqueeze(0)\n",
    "    X_img = X_img.type(torch.FloatTensor)\n",
    "    with torch.no_grad():\n",
    "        ################################################ [TODO] ###################################################\n",
    "        # predict the masks \n",
    "        output_img = net.forward(X_img)\n",
    "        output_img = output_img.squeeze(0)\n",
    "    # For all pixels in predicted mask, set them to 1 if larger than out_threshold. Otherwise set them to 0\n",
    "    return output_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might need to use these functions in the following steps\n",
    "# hwc_to_chw: Convert images from Height*Width*Channels to Channels*Height*Width\n",
    "def hwc_to_chw(img):\n",
    "    return np.transpose(img, axes=[2, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainDataset[0][imgnum]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot original image and mask image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAEc1JREFUeJzt3X2sZHddx/H3Z+bOvbe7rX2wPJS2sa0hJEjUNhvCg0FipZZKWkz4o0S0AgkhWi1GAiVNhPiXiOIjgVRAqzaFyIM0WKQbHmJMbKVd+8gC3dYKS5cWRfuwu3fn3pmvf8xZmL3cu3t/3zlzdtff55Xc3Lkz53fPd34z3zlnzpnvfBURmFl9esc7ADM7Ppz8ZpVy8ptVyslvViknv1mlnPxmlXLym1XKyW9WKSe/WaUWulzZtuXFOP205fKBMW4/mE2IxCce05+SHOWGpVaXjVGpUUH5Y6bktmicGafkdk+5+SDKxyWG8NQzKxxcWd3SyE6T//TTlnnza3cUj4vRweIxSr5eiGHxmN7qam5l8WRqmBKvGaPkC01/3E+NW0vM46B/SmpdB9hWPmhxe2pda9kXjShPtZHK5/6Wf7x7y8t6t9+sUjMlv6TLJX1d0h5J17cVlJnNXzr5JfWBDwCvBl4IvF7SC9sKzMzma5Yt/4uBPRHxSEQMgY8BV7UTlpnN2yzJfy7wram/9zbXmdlJYJbk3+h0wg+dT5L0Fkl3SbrrwEr5EWAzm49Zkn8vcP7U3+cBj61fKCJujIgdEbFj2/LiDKszszbNkvxfAZ4v6UJJi8DVwK3thGVm85b+kE9ErEm6Fvg80Ac+GhEPthaZmc3VTJ/wi4jbgNtaisXMOuRP+JlVyslvVqlOC3s0HtM7uFI8rtcrr9KJWCseA7AwPFQ+aDV3CrOfKH4BYK18XLIWjXHyKdKP8mKnxeSzsX9K+b17ZpyoLgVimHzMRuXbWS0lzo4VVJh6y29WKSe/WaWc/GaVcvKbVcrJb1YpJ79ZpZz8ZpVy8ptVyslvViknv1mlnPxmlXLym1Wq08IeAGXaOI3Ki0QGvVwXncFipiAoV+wxOJQoIgKUuG/ZV/lxL1cgxah8XK+X6w60EOXdiMa9/bl1JTNmOBoUjxltrevWEeTCHjM7Fie/WaWc/GaVmqVd1/mSviRpt6QHJV3XZmBmNl+zHPBbA34nInZJOg24W9LOiPhqS7GZ2Rylt/wRsS8idjWXnwZ243ZdZieNVt7zS7oAuBi4c4Pbptp15U6/mVn7Zk5+SacCnwTeFhFPrb/9yHZd5ec6zWw+Zkp+SQMmiX9zRHyqnZDMrAuzHO0X8BFgd0S8v72QzKwLs2z5Xw78CvBzku5pfq5oKS4zm7NZGnX+C/leEGZ2nPkTfmaV6raqLwLWyiuwFgflFWLq5XZKYrj1qqjDlsa56rxlchVzSrxkh8rnHaBH+XwARKLFmpRbF4PyU8inLiRaYQHPrCW3lyqfj9XE856COfSW36xSTn6zSjn5zSrl5DerlJPfrFJOfrNKOfnNKuXkN6uUk9+sUk5+s0o5+c0q5eQ3q1SnhT1SsLhQXmDSj4PFY3qJFl8AS1opHrOcKCwB2N5/OjUuU/+Srb1OdtAiUw807uXani2MTikeszIoL7QBYDE3kweV+Qq7peIRJe3wvOU3q5ST36xSTn6zSrXx1d19Sf8u6bNtBGRm3Whjy38dk249ZnYSmfV7+88DfhH4cDvhmFlXZt3y/wnwDig4v2BmJ4RZmna8BngiIu4+xnLf79W3fyX3hZVm1r5Zm3ZcKelR4GNMmnf83fqFpnv1bV/u9suCzWxzs7TofldEnBcRFwBXA1+MiDe0FpmZzZXP85tVqpX98Ij4MvDlNv6XmXXDW36zSnXerkuj8qq5/qj8LMFAuRZag9XycUvj8vsEsJTtTpUoLFPHLVUzlYfjRIsvgN62xGN9oLwSEGCcbCk2SoQ4WipPz5L2at7ym1XKyW9WKSe/WaWc/GaVcvKbVcrJb1YpJ79ZpZz8ZpVy8ptVyslvViknv1mlnPxmlXLym1Wq06q+nmAxUV62QKK/36h8DMAgMSNLuQJCBrkQyXR9S7WKI9/jLzMw+w2P/VH5NkyZMjuAWM4NWyufkIjy+xXjra/HW36zSjn5zSo1a9OOMyR9QtLXJO2W9NK2AjOz+Zr1Pf+fAv8UEa+TtAhsayEmM+tAOvkl/QjwCuDXACJiCAzbCcvM5m2W3f6LgO8Cf9V06f2wpO0txWVmczZL8i8AlwAfjIiLgf3A9esXmm7X9cxK8tyWmbVuluTfC+yNiDubvz/B5MXgCNPtuk5d7s+wOjNr0yztur4DfEvSC5qrLgW+2kpUZjZ3sx7t/03g5uZI/yPAG2cPycy6MFPyR8Q9wI6WYjGzDvkTfmaV6rSwJyIYr5Uf8V9M9KeKSN611fJ1jQb7U6tayHWnQkuJdSUrdMbJaeytlo8ZJY8H9xfKB/aWFlPr0v5kkL3y7exgsbw1WEndnLf8ZpVy8ptVyslvViknv1mlnPxmlXLym1XKyW9WKSe/WaWc/GaVcvKbVcrJb1YpJ79ZpZz8ZpXqtKpP6tFbKv9279FqeXUTypbMJcYsnJpaVcT/psaxUD4fkf36xOwzJPGQkajeBBitlm/DRsmSyt5S5o5Br5d8Ps6Rt/xmlXLym1Vq1nZdvy3pQUkPSLpFUq6FqZl1Lp38ks4FfgvYEREvAvrA1W0FZmbzNetu/wJwiqQFJn36Hps9JDPrwizf2/9t4A+BbwL7gCcj4va2AjOz+Zplt/9M4CrgQuB5wHZJb9hguR+06zq0lo/UzFo1y27/zwP/ERHfjYhV4FPAy9YvdES7rqVOP1ZgZkcxS/J/E3iJpG2SxKRd1+52wjKzeZvlPf+dTJpz7gLub/7XjS3FZWZzNmu7rncD724pFjPrkD/hZ1YpJ79Zpbo9/K5INXEbJyqwkh3VGI3K1xXj3CnMUT9XIdZPFL+Nsj33ktWAo0SMkejvB8By+WOm1dyEjCP3mCkzj4lefWjrY7zlN6uUk9+sUk5+s0o5+c0q5eQ3q5ST36xSTn6zSjn5zSrl5DerlJPfrFJOfrNKOfnNKtVtYU/06I2Xiodprbzio5cs7ekPyospegcXU+taG+bGiWH5oMVcK6xeqn8ZZPqejWKQWtNqol3E6lquQGc1yp+/AMNh+X1bWSu/XzHa+vbcW36zSjn5zSp1zOSX9FFJT0h6YOq6syTtlPRQ8/vM+YZpZm3bypb/r4HL1113PfCFiHg+8IXmbzM7iRwz+SPin4Hvrbv6KuCm5vJNwGtbjsvM5iz7nv85EbEPoPn97PZCMrMuzP2A33S7rv1u12V2wsgm/+OSzgFofj+x2YLT7bq2u12X2Qkjm/y3Atc0l68BPtNOOGbWla2c6rsF+FfgBZL2Snoz8PvAqyQ9BLyq+dvMTiLH3A+PiNdvctOlLcdiZh3yJ/zMKuXkN6tUp4ffAxgm+jgtn1Je/TYa517XxolqrwODXJ+pBU5JjVtVecXiUj9X5ThcSLYUWyt/ag37yW3RQnn123gpV1E5OpCs6huUP+81SMxHwRBv+c0q5eQ3q5ST36xSTn6zSjn5zSrl5DerlJPfrFJOfrNKOfnNKuXkN6uUk9+sUk5+s0p1W9ijPsOlM4rH7V87VDxmTU8XjwFIfcvgINe2QGsHUuPWegeLxyyNcq2w1hLtywCWVL5dWcnVHjFeLS/sObiQa0MWi7mU6UX5uFEkCr8Kir685TerlJPfrFJOfrNKZXv1vU/S1yTdJ+nTksrfyJvZcZXt1bcTeFFE/CTwDeBdLcdlZnOW6tUXEbdHxOED43cA580hNjObozbe878J+NxmNx7Rrmsl9113Zta+mZJf0g1MTo3fvNkyR7TrWs6dazaz9qU/5CPpGuA1wKURkfskiJkdN6nkl3Q58E7gZyMi9zE1Mzuusr36/gI4Ddgp6R5JH5pznGbWsmyvvo/MIRYz65A/4WdWqU6r+sbKVW6NxqPEoFzV1jBx7HJxNE6tK5IvvVL5WZP9/dwx2YVUmSOsJMYMh7kWWsPF8se6fyj31F8d5M5YjaO8zdeoVx5jFEyFt/xmlXLym1XKyW9WKSe/WaWc/GaVcvKbVcrJb1YpJ79ZpZz8ZpVy8ptVyslvViknv1mlnPxmleq0qk+Ifur1prxqa5QpKwNQeYXeeC3XZG44Tn6n4Wp5jNHPVTmOS8rEpg3K52Q1+WVww8T8K5K9C0flfQEBxolqwBHllYAlpaLe8ptVyslvVqlUu66p294uKSSdPZ/wzGxesu26kHQ+8Crgmy3HZGYdSLXravwx8A7A39lvdhJKveeXdCXw7Yi4dwvL/qBd18FhZnVmNgfFp/okbQNuAC7byvIRcSNwI8DznnW69xLMThCZLf+PAxcC90p6lEmH3l2SnttmYGY2X8Vb/oi4H3j24b+bF4AdEfFfLcZlZnOWbddlZie5bLuu6dsvaC0aM+uMP+FnVqlOC3sCyHR/GvTK2zit9nN9pjL1L4eSrbBYy732DhMxLiv3UB9czLUiWxiVF7Ks9HLz2BuXF8AcWs61Bhsni7GiX74+9RPr0tafHN7ym1XKyW9WKSe/WaWc/GaVcvKbVcrJb1YpJ79ZpZz8ZpVy8ptVyslvViknv1mlnPxmlXLym1VKEd19rZ6k7wL/ucnNZwMnwrcBOY4jOY4jnehx/FhEPGsr/6DT5D8aSXdFxA7H4TgcRzdxeLffrFJOfrNKnUjJf+PxDqDhOI7kOI70/yaOE+Y9v5l160Ta8ptZhzpNfkmXS/q6pD2Srt/g9iVJH29uv1PSBXOI4XxJX5K0W9KDkq7bYJlXSnpS0j3Nz++2HcfUuh6VdH+znrs2uF2S/qyZk/skXdLy+l8wdT/vkfSUpLetW2Zu87FRC3hJZ0naKemh5veZm4y9plnmIUnXzCGO90n6WjPvn5Z0xiZjj/oYthDHeyR9e2r+r9hk7FHz64dERCc/QB94GLgIWATuBV64bplfBz7UXL4a+Pgc4jgHuKS5fBrwjQ3ieCXw2Y7m5VHg7KPcfgXwOUDAS4A75/wYfYfJueJO5gN4BXAJ8MDUdX8AXN9cvh547wbjzgIeaX6f2Vw+s+U4LgMWmsvv3SiOrTyGLcTxHuDtW3jsjppf63+63PK/GNgTEY9ExBD4GHDVumWuAm5qLn8CuFQq+C7iLYiIfRGxq7n8NLAbOLfNdbTsKuBvYuIO4AxJ58xpXZcCD0fEZh/Eal1s3AJ++nlwE/DaDYb+ArAzIr4XEf8D7AQubzOOiLg9Ig5/B/wdTPpSztUm87EVW8mvI3SZ/OcC35r6ey8/nHTfX6aZ9CeBH51XQM3biouBOze4+aWS7pX0OUk/Ma8YmLQzuF3S3ZLessHtW5m3tlwN3LLJbV3NB8BzImIfTF6smeoNOaXLeQF4E5M9sI0c6zFsw7XN24+PbvI2qHg+ukz+jbbg6081bGWZVkg6Ffgk8LaIeGrdzbuY7Pr+FPDnwD/MI4bGyyPiEuDVwG9IesX6UDcY0/qcSFoErgT+foObu5yPreryuXIDk34zN2+yyLEew1l9kEl37J8G9gF/tFGYG1x31PnoMvn3AudP/X0e8Nhmy0haAE4ntwt0VJIGTBL/5oj41PrbI+KpiHimuXwbMJB0dttxNP//seb3E8Cnmey+TdvKvLXh1cCuiHh8gxg7m4/G44ff2jS/n9hgmU7mpTmQ+Brgl6N5c73eFh7DmUTE4xExiogx8Jeb/P/i+egy+b8CPF/Shc1W5mrg1nXL3AocPmr7OuCLm014VnMM4SPA7oh4/ybLPPfwsQZJL2YyT//dZhzN/94u6bTDl5kcYHpg3WK3Ar/aHPV/CfDk4V3ilr2eTXb5u5qPKdPPg2uAz2ywzOeByySd2ewGX9Zc1xpJlwPvBK6MiAObLLOVx3DWOKaP8fzSJv9/K/l1pDaOUBYcybyCydH1h4Ebmut+j8nkAiwz2e3cA/wbcNEcYvgZJrtD9wH3ND9XAG8F3toscy3wIJMjpncAL5vTfFzUrOPeZn2H52Q6FgEfaObsfmDHHOLYxiSZT5+6rpP5YPKCsw9YZbL1ejOT4zxfAB5qfp/VLLsD+PDU2Dc1z5U9wBvnEMceJu+jDz9PDp+Jeh5w29Eew5bj+Nvmsb+PSUKfsz6OzfLraD/+hJ9ZpfwJP7NKOfnNKuXkN6uUk9+sUk5+s0o5+c0q5eQ3q5ST36xS/wfcPRLQNG+z8AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot original images and masks\n",
    "for i in range (1):\n",
    "    img_num=32109\n",
    "    test_img = trainDataset[0][img_num]\n",
    "    #lx = np.gradient(test_img0, axis=0)\n",
    "    #ly = np.gradient(test_img0, axis=1)\n",
    "    #t=np.sum(lx**2+ly**2)/(16*16)\n",
    "    #if t>0.1:\n",
    "     #   print(i)\n",
    "    test_img_show = np.transpose(test_img, axes=[1, 2, 0]) + 128/255\n",
    "    plt.imshow(test_img_show)\n",
    "    plt.figure()\n",
    "    #Predict a new image\n",
    "    pred_img = predict_img(net, test_img)\n",
    "    pred_img_show = np.transpose(pred_img, axes=[1, 2, 0]) +128/255.\n",
    "    plt.imshow(pred_img_show)\n",
    "    plt.figure()\n",
    "# count = 0;\n",
    "# for i in range(20,30):\n",
    "#     imgnum = i*10\n",
    "#     test_img = trainDataset[0][imgnum]\n",
    "#     test_img0=test_img+128/255.\n",
    "#     lx = np.gradient(test_img0, axis=0)\n",
    "#     ly = np.gradient(test_img0, axis=1)\n",
    "#     t=np.sum(lx**2+ly**2)/(16*16)\n",
    "#     if t>0.01:\n",
    "#         test_img_show = np.transpose(test_img, axes=[1, 2, 0]) + 128/255\n",
    "#         plt.imshow(test_img_show)\n",
    "#         plt.figure()\n",
    "#     #Predict a new image\n",
    "#         pred_img = predict_img(net, test_img)\n",
    "#         pred_img_show = np.transpose(pred_img, axes=[1, 2, 0]) +128/255\n",
    "#         plt.imshow(pred_img_show)\n",
    "#         plt.figure()\n",
    "#         count= count+1\n",
    "#     if count == 10:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 50\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "# val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "# for i, b in enumerate(train_loader):\n",
    "#     ################################################ [TODO] ###################################################\n",
    "#     # Get images and masks from each batch\n",
    "#     imgs = b['img']\n",
    "#     label = b['label']\n",
    "#     Gx_in, Gx_target, Gy_in, Gy_target= grad_loss(imgs,imgs)\n",
    "#     print(Gx_in.shape)\n",
    "#     save = imgs[49]\n",
    "#     if i == 1:\n",
    "#         break\n",
    "# print(Gx_in)\n",
    "# test_img_show = np.transpose(Gx_in, axes=[1, 2, 0]) + 128/255\n",
    "# plt.imshow(test_img_show)\n",
    "# plt.figure()       \n",
    "# test_img_show = np.transpose(save, axes=[1, 2, 0]) + 128/255\n",
    "# plt.imshow(test_img_show)\n",
    "# plt.figure()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "###ORIGINAL IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Coeff: tensor([0.0097], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "val_dice = eval_net(net, val_loader)\n",
    "print('Validation Dice Coeff: {}'.format(val_dice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = tf.constant([[[[1, 2, 3], [4, 5, 6]],[[1, 2, 3], [4, 5, 6]]],\n",
    "#                 [[[1, 2, 3], [4, 5, 6]],[[1, 2, 3], [4, 5, 6]]]])\n",
    "# print(t.shape)\n",
    "# paddings = tf.constant([[0, 0,],[0, 0,],[1, 1,], [2, 2]])\n",
    "# p = tf.pad(t, paddings, \"SYMMETRIC\")\n",
    "# print(p.shape)\n",
    "# print(tf.contrib.framework.is_tensor(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_20 = glob.glob('./Data/Validation/validation_20')\n",
    "train_20 = glob.glob('./Data/Train/train_20')\n",
    "test_20 = glob.glob('./Data/Test/test_20')\n",
    "valiDataset_20 = []\n",
    "trainDataset_20 = []\n",
    "testDataset_20 = []\n",
    "#Loads the following in a list of (3, 16,16) and (3, 20,20)\n",
    "for i in range(len(validation_20)):\n",
    "    f=open(validation_20[i],'rb')\n",
    "    b=pickle.load(f)\n",
    "    #flip the data dimension\n",
    "    b = np.transpose(b, axes=[3, 2, 0, 1])\n",
    "    f.close()\n",
    "    valiDataset_20.append(b)\n",
    "    \n",
    "for i in range(len(test_20)):\n",
    "    f=open(test_20[i],'rb')\n",
    "    b=pickle.load(f)\n",
    "    b = np.transpose(b, axes=[3, 2, 0, 1]) \n",
    "#     b=b.astype(int)\n",
    "    f.close()\n",
    "    testDataset_20.append(b)\n",
    "    \n",
    "for i in range(len(train_20)):\n",
    "    f=open(train_20[i],'rb')\n",
    "    b=pickle.load(f)\n",
    "    b = (np.transpose(b, axes=[3, 2, 0, 1])-128)/255.\n",
    "    f.close()\n",
    "    trainDataset_20.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    img_num =56\n",
    "    a = trainDataset_20[0][img_num]\n",
    "    a = np.transpose(a, axes = [1,2,0])+128/255.\n",
    "    plt.imshow(a)\n",
    "    plt.title(\"sample image\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuxuanqi/anaconda3/lib/python3.7/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type UNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/yuxuanqi/anaconda3/lib/python3.7/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type single_conv. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/yuxuanqi/anaconda3/lib/python3.7/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type resize. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/yuxuanqi/anaconda3/lib/python3.7/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type upT. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "###Saving Parameters 3 only MSE 4 MSE + GRad loss, 5 grad loss only\n",
    "PATH = './model/modelparamsGradComp95.pth'\n",
    "torch.save(net, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading parameters\n",
    "PATH = './model/modelparamsGradComp95.pth'\n",
    "net = torch.load(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 16, 3)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "running_mean should contain 16 elements not 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-b7dd0d20fcff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mpred_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-d93a2dd2a935>\u001b[0m in \u001b[0;36mpredict_img\u001b[0;34m(net, full_img)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m################################################ [TODO] ###################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# predict the masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moutput_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0moutput_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# For all pixels in predicted mask, set them to 1 if larger than out_threshold. Otherwise set them to 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-e76ef835a19a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m#ENCODE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#16x16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#12x12\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#8x8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e260eca2c1aa>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m#paddings = tf.constant([[0, 0,],[0, 0,],[1, 1,], [1, 1]])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m#x = tf.pad(x, paddings, \"SYMMETRIC\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1252\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1253\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m     )\n\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: running_mean should contain 16 elements not 3"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "q=trainDataset_20[0][56]\n",
    "q = np.transpose(q, axes = [1,2,0])+128/255.\n",
    "m=np.zeros((20,20,3))\n",
    "for i in range(20):\n",
    "    m[:,i,:]=q[:,19-i,:]\n",
    "m0=m[4:20,4:20,:]\n",
    "plt.imshow(m0)\n",
    "print(m0.shape)\n",
    "pred_img = predict_img(net, m0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# trainDataset_new = np.zeros((70000,3,20,20))\n",
    "# for k in range (70000):\n",
    "#     a = trainDataset_20[0][k]\n",
    "#     a = np.transpose(a, axes = [1,2,0])\n",
    "#     b0= trainDataset[0][k]\n",
    "#     pred_img = predict_img(net, b0)\n",
    "#     b = np.transpose(pred_img, axes=[1, 2, 0])\n",
    "#     a[4:20,4:20,:]=b\n",
    "#     a=hwc_to_chw(a)\n",
    "#     trainDataset_new[k,:,:,:]=a\n",
    "# print(trainDataset_new.shape)\n",
    "# trainDataset_20_new=[]\n",
    "# for k in range(70000):\n",
    "#     a=trainDataset_new\n",
    "#     trainDataset_20_new.append(a)\n",
    "# f2=open('train_20_new_2','rb')\n",
    "# train_20_new=pickle.load(f2)\n",
    "# f2.close\n",
    "# img_num =999\n",
    "# a = train_20_new[0][img_num]\n",
    "# a = np.transpose(a, axes = [1,2,0])+128/255.\n",
    "# plt.imshow(a)\n",
    "# plt.title(\"sample image\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_num =6666\n",
    "a = valiDataset_20[0][img_num]\n",
    "a = np.transpose(a, axes = [1,2,0])+128/255.\n",
    "plt.imshow(a)\n",
    "plt.title(\"sample image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 20, 20)\n"
     ]
    }
   ],
   "source": [
    "ValidationDataset_new = np.zeros((10000,3,20,20))\n",
    "for k in range (10000):\n",
    "    a = valiDataset_20[0][k]\n",
    "    a = np.transpose(a, axes = [1,2,0])\n",
    "    b0= valiDataset[0][k]\n",
    "    pred_img = predict_img(net, b0)\n",
    "    b = np.transpose(pred_img, axes=[1, 2, 0])\n",
    "    a[4:20,4:20,:]=b\n",
    "    a=hwc_to_chw(a)\n",
    "    ValidationDataset_new[k,:,:,:]=a\n",
    "print(a.shape)\n",
    "ValidationDataset_20_new=[]\n",
    "for k in range(10000):\n",
    "    a=ValidationDataset_new\n",
    "    ValidationDataset_20_new.append(a)\n",
    "f5=open('validation_20_new_2','wb')\n",
    "pickle.dump(ValidationDataset_20_new,f5)\n",
    "f5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f2=open('validation_20_new_2','rb')\n",
    "train_20_new=pickle.load(f2)\n",
    "f2.close\n",
    "img_num =6666\n",
    "a = train_20_new[0][img_num]\n",
    "a = np.transpose(a, axes = [1,2,0])+128/255.\n",
    "plt.imshow(a)\n",
    "plt.title(\"sample image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 20, 20)\n"
     ]
    }
   ],
   "source": [
    "testDataset_new = np.zeros((10000,3,20,20))\n",
    "for k in range (10000):\n",
    "    a = testDataset_20[0][k]\n",
    "    a = np.transpose(a, axes = [1,2,0])\n",
    "    b0= testDataset[0][k]\n",
    "    pred_img = predict_img(net, b0)\n",
    "    b = np.transpose(pred_img, axes=[1, 2, 0])\n",
    "    a[4:20,4:20,:]=b\n",
    "    a=hwc_to_chw(a)\n",
    "    testDataset_new[k,:,:,:]=a\n",
    "print(a.shape)\n",
    "testDataset_20_new=[]\n",
    "for k in range(10000):\n",
    "    a=testDataset_new\n",
    "    testDataset_20_new.append(a)\n",
    "f6=open('test_20_new_2','wb')\n",
    "pickle.dump(testDataset_20_new,f6)\n",
    "f6.close()\n",
    "# img_num=777\n",
    "# img=testDataset_20_new[0][img_num]\n",
    "# img=np.transpose(img, axes = [1,2,0])+128/255.\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
